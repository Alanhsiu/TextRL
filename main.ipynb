{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pfrl@git+https://github.com/voidful/pfrl.git\n",
    "%pip install textrl==0.2.15\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pfrl\n",
    "from textrl import TextRLEnv,TextRLActor\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer, AutoModelWithLMHead\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='')\n",
    "from NISQA.nisqa.NISQA_model import nisqaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\")\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = pipeline('sentiment-analysis',model=\"cardiffnlp/twitter-roberta-base-sentiment\",tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment\",device=0,return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_logger = logging.getLogger('transformers')\n",
    "transformers_logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment(\"dogecoin is bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment(\"dogecoin is bad\")[0][0]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRLEnv(TextRLEnv):\n",
    "    def __init__(self, model, tokenizer, device=\"cuda\", observation_input=[], max_length=100, compare_sample=2):\n",
    "        super().__init__(model, tokenizer, observation_input, max_length, compare_sample)\n",
    "        self.device = torch.device(device)\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.observation_input = observation_input\n",
    "        \n",
    "        \n",
    "    def get_reward(self, input_item, predicted_list, finish): # predicted will be the list of predicted token\n",
    "        args = {\n",
    "            'mode': 'predict_file',  # For example, 'predict_file', 'predict_dir', or 'predict_csv'\n",
    "            'pretrained_model': '/home/b0990106x/TextRL/NISQA/weights/nisqa.tar',  # Name of the pretrained model file\n",
    "            'deg': '/home/b0990106x/TextRL/NISQA/wav/test.wav',  # Path to the speech file if mode is predict_file\n",
    "            'data_dir': None,  # Folder with speech files if mode is predict_dir\n",
    "            'output_dir': '/home/b0990106x/TextRL/NISQA/result',  # Folder to output results.csv\n",
    "            'csv_file': None,  # Name of the csv file if mode is predict_csv\n",
    "            'csv_deg': None,  # Column name in csv with file names/paths if mode is predict_csv\n",
    "            'num_workers': 0,  # Number of workers for PyTorch's DataLoader\n",
    "            'bs': 1,  # Batch size for predicting\n",
    "            'ms_channel': None,  # Audio channel in case of a stereo file, if needed\n",
    "        }\n",
    "\n",
    "        if args['mode'] == 'predict_file':\n",
    "            if args['deg'] is None:\n",
    "                raise ValueError('--deg argument with path to input file needed')\n",
    "        elif args['mode'] == 'predict_dir':\n",
    "            if args['data_dir'] is None:\n",
    "                raise ValueError('--data_dir argument with folder with input files needed')\n",
    "        elif args['mode'] == 'predict_csv':\n",
    "            if args['csv_file'] is None:\n",
    "                raise ValueError('--csv_file argument with csv file name needed')\n",
    "            if args['csv_deg'] is None:\n",
    "                raise ValueError('--csv_deg argument with csv column name of the filenames needed')\n",
    "            if args['data_dir'] is None:\n",
    "                args['data_dir'] = ''\n",
    "        else:\n",
    "                raise NotImplementedError('--mode given not available')\n",
    "        args['tr_bs_val'] = args['bs']\n",
    "        args['tr_num_workers'] = args['num_workers']\n",
    "        \n",
    "        nisqa = nisqaModel(args)\n",
    "        prediction = nisqa.predict()\n",
    "        reward = int(prediction['mos_pred'].iloc[0])\n",
    "        return reward\n",
    "\n",
    "    # def step(self, action):\n",
    "    #     predicted, finish, predicted_str = self.predict(vocab_id=action)\n",
    "    #     reward = self.get_reward(self.input_item, predicted, finish)\n",
    "    #     # Assuming `action` is an index representing a choice of instruction or modification\n",
    "\n",
    "    #     # Prepare the model input based on the chosen action\n",
    "    #     # This involves selecting the instruction and possibly modifying voice features\n",
    "    #     instruction, transcription, src_encodec_ids = self.prepare_input(action)\n",
    "\n",
    "    #     # Generate voice output using the model\n",
    "    #     # For simplicity, we're directly using a method that might encapsulate the model interaction\n",
    "    #     layer_list = self.generate_voice(instruction, transcription, src_encodec_ids)\n",
    "\n",
    "    #     # Convert model output to audio and evaluate it to calculate the reward\n",
    "    #     encodec_code = convert_to_encode_code(self.tokenizer, layer_list)\n",
    "    #     audio = synthesize_audio(encodec_code, self.device)\n",
    "    #     reward = self.evaluate_audio(audio)  # This would be an external or predefined method\n",
    "\n",
    "    #     # Check if the episode is done based on your criteria\n",
    "    #     done = self.check_done()\n",
    "\n",
    "    #     # Optionally update the environment state here\n",
    "\n",
    "    #     return self.get_observation(), reward, done, {}\n",
    "    \n",
    "    # def predict(self, vocab_id):\n",
    "    #     predicted_list = {}\n",
    "    #     predicted_list_end = {}\n",
    "    #     with torch.inference_mode():\n",
    "    #         for i, (v_id, predicted, predicted_end) in enumerate(zip(vocab_id, self.predicted, self.predicted_end)):\n",
    "    #             predicted_list_end[i] = False\n",
    "    #             if not predicted_end:\n",
    "    #                 pred_word = self.actions[v_id]\n",
    "    #                 if pred_word in self.gen_stop_toks \\\n",
    "    #                         or len(pred_word) < 1 \\\n",
    "    #                         or len(predicted) > self.env_max_length:\n",
    "    #                     predicted_list_end[i] = True\n",
    "    #                     predicted_list[i] = [pred_word]\n",
    "    #                 else:\n",
    "    #                     predicted_list[i] = [pred_word]\n",
    "    #             else:\n",
    "    #                 predicted_list_end[i] = True\n",
    "    #                 predicted_list[i] = ['']\n",
    "\n",
    "    #         for i, (l, e) in enumerate(zip(predicted_list.values(), predicted_list_end.values())):\n",
    "    #             self.predicted[i] = self.predicted[i] + l\n",
    "    #             self.predicted_end[i] = e\n",
    "\n",
    "    #         return self.predicted, all(self.predicted_end), [self.tokenizer.convert_tokens_to_string(i) for i in\n",
    "    #                                                          self.predicted]\n",
    "    # def reset(self):\n",
    "    #     # Reset or initialize the environment state\n",
    "    #     # This could involve loading or selecting a new dataset entry for instructions and voice features\n",
    "\n",
    "    #     self.current_instruction, self.current_transcription, self.current_src_encodec_ids = self.select_new_instruction()\n",
    "\n",
    "    #     # Prepare the initial observation based on the new instruction\n",
    "    #     # This step depends on how you define an observation in your environment\n",
    "    #     initial_observation = self.prepare_observation(self.current_instruction, self.current_transcription)\n",
    "\n",
    "    #     return initial_observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"lca0503/soxdata_encodec\")\n",
    "# dataset.save_to_disk(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [{}]\n",
    "input_audio = \"\"\n",
    "\n",
    "observation_list = [{'input':'i think dogecoin is'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MyRLEnv(model, tokenizer, observation_input=observation_list, compare_sample=1)\n",
    "actor = TextRLActor(env,model,tokenizer,optimizer='adamw',\n",
    "                    temperature=0.8,\n",
    "                    top_k=100,\n",
    "                    top_p=0.85,)\n",
    "agent = actor.agent_ppo(update_interval=50, minibatch_size=3, epochs=10,lr=3e-4)\n",
    "print(actor.predict(observation_list[0]))\n",
    "\n",
    "pfrl.experiments.train_agent_with_evaluation(\n",
    "    agent,\n",
    "    env,\n",
    "    steps=3000,\n",
    "    eval_n_steps=None,\n",
    "    eval_n_episodes=1,       \n",
    "    train_max_episode_len=100,  \n",
    "    eval_interval=10,\n",
    "    outdir='checkpoint', \n",
    ")\n",
    "agent.load(\"./checkpoint/best\")\n",
    "print(actor.predict(observation_list[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
