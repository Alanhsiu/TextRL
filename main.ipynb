{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pfrl@git+https://github.com/voidful/pfrl.git\n",
    "# %pip install textrl==0.2.15\n",
    "# %pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(59481, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(59481, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(59481, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=59481, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\")\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pfrl\n",
    "from textrl import TextRLEnv,TextRLActor\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer, AutoModelWithLMHead\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "from NISQA.nisqa.NISQA_model import nisqaModel\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/work/b0990106x/TextRL'\n",
    "\n",
    "# print(\"model\", model)\n",
    "# print(\"tokenizer\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RL Environment: NISQA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.2.1 available.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import logging\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "from torch import autocast\n",
    "from vc.wav_to_arrow import process_audio\n",
    "\n",
    "class VcRLEnv(gym.Env):\n",
    "    def __init__(self, model, tokenizer, observation_input=[], max_length=100, compare_sample=2, unfreeze_layer_from_past=0, env_input_dir=None, env_output_dir=None, instruction=\"\", transcription=\"\"):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.observation_space = observation_input\n",
    "        self.compare_sample = compare_sample\n",
    "        self.unfreeze_layer_from_past = 1 if unfreeze_layer_from_past else 0\n",
    "        self.env_max_length = min(max(self.model.config.max_length, self.tokenizer.model_max_length), max_length)\n",
    "        self.env_input_dir = env_input_dir\n",
    "        self.env_output_dir = env_output_dir\n",
    "        self.instruction = instruction\n",
    "        self.transcription = transcription\n",
    "        self.reset()\n",
    "        \n",
    "        self.gen_stop_toks = []\n",
    "        logging.disable(sys.maxsize)\n",
    "        if self.tokenizer.sep_token:\n",
    "            self.gen_stop_toks.append(self.tokenizer.sep_token)\n",
    "        if self.tokenizer.eos_token:\n",
    "            self.gen_stop_toks.append(self.tokenizer.eos_token)\n",
    "        logging.disable(logging.NOTSET)\n",
    "    \n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        predicted, finish, predicted_str= self._predict(vocab_id=action)\n",
    "        reward = self.get_reward(self.input_item, predicted, finish)\n",
    "        self.predicted = predicted\n",
    "        return self._get_obs(predicted), reward, finish, {\"predicted_str\": predicted_str}\n",
    "\n",
    "    \n",
    "    def get_reward(self, input_path=None, output_dir=None, count=0): # predicted will be the list of predicted token\n",
    "        args = {\n",
    "            'mode': 'predict_file', \n",
    "            'pretrained_model': f'{base_path}/NISQA/weights/nisqa.tar', \n",
    "            'deg': f'{base_path}/output/{count}.wav', \n",
    "            'data_dir': None, \n",
    "            'output_dir': f'{base_path}/NISQA/result',\n",
    "            'csv_file': None, \n",
    "            'csv_deg': None,  \n",
    "            'num_workers': 0, \n",
    "            'bs': 1,\n",
    "            'ms_channel': None\n",
    "        }\n",
    "\n",
    "        if input_path is not None:\n",
    "            args['deg'] = input_path\n",
    "\n",
    "        args['tr_bs_val'] = args['bs']\n",
    "        args['tr_num_workers'] = args['num_workers']\n",
    "        \n",
    "        nisqa = nisqaModel(args)\n",
    "        prediction = nisqa.predict()\n",
    "        reward = float(prediction['mos_pred'].iloc[0])\n",
    "        print(\"count\", count, \"reward\", reward)\n",
    "        return reward\n",
    "    \n",
    "        \n",
    "    def gat_obs_input(self, input_item):\n",
    "        return input_item['input']\n",
    "    \n",
    "    @autocast('cuda')\n",
    "    def reset(self, input_item=None):\n",
    "        self.predicted = [[]] * self.compare_sample\n",
    "        self.predicted_end = [False] * self.compare_sample\n",
    "        self.input_item = {\"input\": \"\"}\n",
    "        if input_item is None:\n",
    "            self.input_item = random.choice(self.observation_space)\n",
    "        else:\n",
    "            self.input_item = input_item\n",
    "        return self._get_obs(self.predicted)\n",
    "    \n",
    "    @autocast('cuda')\n",
    "    def _get_obs(self, predicted=[]):\n",
    "        # # backup\n",
    "        # audio_path = f\"{self.env_input_dir}/{count}.wav\"\n",
    "        # process_audio(source_audio_path=audio_path, output_dir = self.env_output_dir, temp_dir=\"/work/b0990106x/TextRL/vc/data/temp\", instruction=self.instruction, transcription=self.transcription)\n",
    "        # return count\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            obs_list = []\n",
    "            for p_text in predicted:\n",
    "                p_text_str = self.tokenizer.convert_tokens_to_string(p_text)\n",
    "                if self.model.__class__.__name__ == 'OPTForCausalLM':\n",
    "                    feature_dict = self.tokenizer([[self.gat_obs_input(self.input_item), p_text_str]],\n",
    "                                                  return_tensors='pt',\n",
    "                                                  return_token_type_ids=False,\n",
    "                                                  add_special_tokens=False).to(self.model.device)\n",
    "                    with torch.cuda.amp.autocast(enabled=False):\n",
    "                        prediction = self.model(**feature_dict, output_hidden_states=True)\n",
    "                    outputs = prediction.hidden_states[-self.unfreeze_layer_from_past][:, -1, :]\n",
    "                else:\n",
    "                    if len([k for k, v in self.model.named_parameters() if 'decoder' in k]) > 0:\n",
    "                        feature_dict = self.tokenizer([self.gat_obs_input(self.input_item)],\n",
    "                                                      return_tensors='pt',\n",
    "                                                      return_token_type_ids=False,\n",
    "                                                      add_special_tokens=True).to(self.model.device)\n",
    "                        if len(p_text) > 0:\n",
    "                            decoder_input_ids = [self.model.config.decoder_start_token_id] + \\\n",
    "                                                self.tokenizer.convert_tokens_to_ids(p_text)\n",
    "                            dec_input = torch.tensor([decoder_input_ids]).to(self.model.device)\n",
    "                            feature_dict['decoder_input_ids'] = dec_input\n",
    "                        else:\n",
    "                            feature_dict['decoder_input_ids'] = torch.tensor(\n",
    "                                [[self.model.config.decoder_start_token_id]]).to(self.model.device)\n",
    "                        with torch.cuda.amp.autocast(enabled=False):\n",
    "                            prediction = self.model(**feature_dict, output_hidden_states=True)\n",
    "                        outputs = prediction.decoder_hidden_states[-self.unfreeze_layer_from_past].squeeze(0)\n",
    "                    else:\n",
    "                        if self.model.__class__.__name__ == 'DistributedBloomForCausalLM':\n",
    "                            with self.model.inference_session(max_length=self.env_max_length) as sess:\n",
    "                                feature_dict = self.tokenizer([[self.gat_obs_input(self.input_item), p_text_str]],\n",
    "                                                              return_tensors='pt',\n",
    "                                                              return_token_type_ids=False,\n",
    "                                                              add_special_tokens=False).to(self.model.device)\n",
    "                                embs = self.model.transformer.word_embeddings(feature_dict.input_ids)\n",
    "                                embs = self.model.transformer.word_embeddings_layernorm(embs)\n",
    "                                h = sess.step(embs)\n",
    "                                outputs = self.model.transformer.ln_f(h[:, -1])\n",
    "                        else:\n",
    "                            feature_dict = self.tokenizer([[self.gat_obs_input(self.input_item), p_text_str]],\n",
    "                                                          return_tensors='pt',\n",
    "                                                          return_token_type_ids=False,\n",
    "                                                          add_special_tokens=False).to(self.model.device)\n",
    "                            prediction = self.model(**feature_dict, output_hidden_states=True)\n",
    "                            outputs = prediction.hidden_states[-self.unfreeze_layer_from_past].squeeze(0)\n",
    "                obs_list.append(outputs.data[-1])\n",
    "            return (torch.stack(obs_list))\n",
    "        \n",
    "        \n",
    "    def _predict(self, vocab_id): \n",
    "        predicted_list = {}\n",
    "        predicted_list_end = {} \n",
    "        with torch.inference_mode():\n",
    "            for i, (v_id, predicted, predicted_end) in enumerate(zip(vocab_id, self.predicted, self.predicted_end)):\n",
    "                predicted_list_end[i] = False\n",
    "                if not predicted_end:\n",
    "                    pred_word = self.actions[v_id]\n",
    "                    if pred_word in self.gen_stop_toks \\\n",
    "                            or len(pred_word) < 1 \\\n",
    "                            or len(predicted) > self.env_max_length:\n",
    "                        predicted_list_end[i] = True\n",
    "                        predicted_list[i] = [pred_word]\n",
    "                    else:\n",
    "                        predicted_list[i] = [pred_word]\n",
    "                else:\n",
    "                    predicted_list_end[i] = True\n",
    "                    predicted_list[i] = ['']\n",
    "\n",
    "            for i, (l, e) in enumerate(zip(predicted_list.values(), predicted_list_end.values())):\n",
    "                self.predicted[i] = self.predicted[i] + l\n",
    "                self.predicted_end[i] = e\n",
    "\n",
    "            return self.predicted, all(self.predicted_end), [self.tokenizer.convert_tokens_to_string(i) for i in\n",
    "                                                             self.predicted]\n",
    "\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RL Agent: Text-Instruction-Guided Voice Conversion Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import autocast\n",
    "from transformers import (AutoTokenizer, BartForConditionalGeneration,\n",
    "                          BatchEncoding)\n",
    "from vc.trainer_encodec_vc_inference import cascade_ar_nar, convert_to_encode_code,synthesize_audio\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import textrl.actor\n",
    "\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pfrl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pfrl.agents.ppo import _elementwise_clip\n",
    "from pfrl.utils.mode_of_distribution import mode_of_distribution\n",
    "from torch import autocast\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "def get_modulelist_pos(model):\n",
    "    module_list_pos = 0\n",
    "    for ids, i in enumerate(list(model.children())):\n",
    "        if isinstance(i, torch.nn.ModuleList):\n",
    "            module_list_pos = ids\n",
    "    return module_list_pos\n",
    "\n",
    "\n",
    "class HFModelListModule(torch.nn.Module):\n",
    "    def __init__(self, module_list):\n",
    "        super(HFModelListModule, self).__init__()\n",
    "        self.module_list = module_list\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        for module in self.module_list:\n",
    "            hidden = module(hidden)[0]\n",
    "        return hidden\n",
    "\n",
    "class VcPPOAgent(pfrl.agents.PPO):\n",
    "    def _update_if_dataset_is_ready(self):\n",
    "        dataset_size = (\n",
    "                sum(len(episode) for episode in self.memory)\n",
    "                + len(self.last_episode)\n",
    "                + (\n",
    "                    0\n",
    "                    if self.batch_last_episode is None\n",
    "                    else sum(len(episode) for episode in self.batch_last_episode)\n",
    "                )\n",
    "        )\n",
    "        if dataset_size >= self.update_interval:\n",
    "            self._flush_last_episode()\n",
    "            if self.recurrent:\n",
    "                dataset = pfrl.agents.ppo._make_dataset_recurrent(\n",
    "                    episodes=self.memory,\n",
    "                    model=self.model,\n",
    "                    phi=self.phi,\n",
    "                    batch_states=self.batch_states,\n",
    "                    obs_normalizer=self.obs_normalizer,\n",
    "                    gamma=self.gamma,\n",
    "                    lambd=self.lambd,\n",
    "                    max_recurrent_sequence_len=self.max_recurrent_sequence_len,\n",
    "                    device=self.device,\n",
    "                )\n",
    "                self._update_recurrent(dataset)\n",
    "            else:\n",
    "                dataset = pfrl.agents.ppo._make_dataset(\n",
    "                    episodes=self.memory,\n",
    "                    model=self.model,\n",
    "                    phi=self.phi,\n",
    "                    batch_states=self.batch_states,\n",
    "                    obs_normalizer=self.obs_normalizer,\n",
    "                    gamma=self.gamma,\n",
    "                    lambd=self.lambd,\n",
    "                    device=self.device,\n",
    "                )\n",
    "                assert len(dataset) == dataset_size\n",
    "                self._update(dataset)\n",
    "            self.explained_variance = self._compute_explained_variance(\n",
    "                list(itertools.chain.from_iterable(self.memory))\n",
    "            )\n",
    "            self.memory = []\n",
    "\n",
    "    def _compute_explained_variance(self, transitions):\n",
    "        \"\"\"Compute 1 - Var[return - v]/Var[return].\n",
    "\n",
    "        This function computes the fraction of variance that value predictions can\n",
    "        explain about returns.\n",
    "        \"\"\"\n",
    "        t = np.array([tr[\"v_teacher\"] for tr in transitions])\n",
    "        y = np.array([tr[\"v_pred\"] for tr in transitions])\n",
    "        vart = np.var(t)\n",
    "        if vart == 0:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return float(1 - np.var(np.average(t) - y) / vart)\n",
    "\n",
    "    def batch_act(self, batch_obs):\n",
    "        if self.training:\n",
    "            return self._batch_act_train(batch_obs)\n",
    "        else:\n",
    "            return self._batch_act_eval(batch_obs)\n",
    "\n",
    "    @autocast('cuda')\n",
    "    def _batch_act_eval(self, batch_obs):\n",
    "        assert not self.training\n",
    "        b_state = self.batch_states(batch_obs, self.device, self.phi)\n",
    "\n",
    "        if self.obs_normalizer:\n",
    "            b_state = self.obs_normalizer(b_state, update=False)\n",
    "\n",
    "        with torch.no_grad(), pfrl.utils.evaluating(self.model):\n",
    "            action_distrib, _ = self.model(b_state)\n",
    "            if self.act_deterministically:\n",
    "                action = mode_of_distribution(action_distrib).cpu().numpy()\n",
    "            else:\n",
    "                action = action_distrib.sample().cpu().numpy()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def _lossfun(\n",
    "            self, entropy, vs_pred, log_probs, vs_pred_old, log_probs_old, advs, vs_teacher\n",
    "    ):\n",
    "        prob_ratio = torch.exp(log_probs - log_probs_old)\n",
    "        loss_policy = -torch.mean(\n",
    "            torch.min(\n",
    "                (prob_ratio * advs),\n",
    "                torch.clamp(prob_ratio, 1 - self.clip_eps, 1 + self.clip_eps) * advs,\n",
    "            ),\n",
    "        )\n",
    "        if self.clip_eps_vf is None:\n",
    "            loss_value_func = F.mse_loss(vs_pred.squeeze(), vs_teacher.squeeze())\n",
    "        else:\n",
    "            clipped_vs_pred = _elementwise_clip(\n",
    "                vs_pred,\n",
    "                vs_pred_old - self.clip_eps_vf,\n",
    "                vs_pred_old + self.clip_eps_vf,\n",
    "            )\n",
    "            loss_value_func = torch.mean(\n",
    "                torch.max(\n",
    "                    F.mse_loss(vs_pred.squeeze(), vs_teacher, reduction=\"none\"),\n",
    "                    F.mse_loss(clipped_vs_pred.squeeze(), vs_teacher, reduction=\"none\"),\n",
    "                )\n",
    "            )\n",
    "        loss_entropy = -torch.mean(entropy)\n",
    "\n",
    "        self.value_loss_record.append(float(loss_value_func))\n",
    "        self.policy_loss_record.append(float(loss_policy))\n",
    "        loss = (\n",
    "                loss_policy\n",
    "                + self.value_func_coef * loss_value_func\n",
    "                + self.entropy_coef * loss_entropy\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "class VcActor():\n",
    "    def __init__(self, env, model, tokenizer,ar_checkpoint, nar_checkpoint, input_dir, output_dir, device, observation_input=[], max_length=100, compare_sample=2, optimizer='sgd', gpu_id=0, unfreeze_layer_from_past=0,\n",
    "                 act_deterministically=True,\n",
    "                 temperature=1.0,\n",
    "                 top_k=0,\n",
    "                 top_p=1.0):\n",
    "        \n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.observation_input = observation_input\n",
    "        self.ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "        self.ar_model = BartForConditionalGeneration.from_pretrained(ar_checkpoint)\n",
    "        self.nar_tokenizer = AutoTokenizer.from_pretrained(nar_checkpoint)\n",
    "        self.nar_model = NARBartForConditionalGeneration.from_pretrained(nar_checkpoint)\n",
    "        self.gpu_id = gpu_id\n",
    "        # self.device = torch.device(device)\n",
    "        self.device = torch.device(\"cuda:{}\".format(gpu_id))\n",
    "        self.ar_model.to(self.device)\n",
    "        self.nar_model.to(self.device)\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # new added\n",
    "        self.act_deterministically = act_deterministically\n",
    "        self.temperature = temperature\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.optimizer = optimizer\n",
    "        self.unfreeze_layer_from_past = unfreeze_layer_from_past\n",
    "\n",
    "        parents = [parent[0] for parent in model.named_children()]\n",
    "        if 'transformer' in parents:  # gpt2/bloom:\n",
    "            transformers_model = model.transformer\n",
    "        elif 'model' in parents:  # bart\n",
    "            transformers_model = model.model\n",
    "        elif 'decoder' in parents:  # t5\n",
    "            transformers_model = model.decoder\n",
    "        else:\n",
    "            raise ValueError('model not supported')\n",
    "\n",
    "        if unfreeze_layer_from_past > 0:\n",
    "            self.middle_model = HFModelListModule(list(transformers_model.children())\n",
    "                                                  [get_modulelist_pos(transformers_model)]\n",
    "                                                  [-self.unfreeze_layer_from_past:])\n",
    "            self.remaining_model = torch.nn.Sequential(\n",
    "                *list(transformers_model.children())[get_modulelist_pos(transformers_model) + 1:])\n",
    "        else:\n",
    "            self.middle_model = torch.nn.Sequential()\n",
    "            self.remaining_model = torch.nn.Sequential()\n",
    "\n",
    "    @autocast('cuda')\n",
    "    def predict(self, input_item):\n",
    "        t = 0 \n",
    "        with torch.inference_mode():\n",
    "            # use the model to predict the next wav\n",
    "            with self.agent.eval_mode():\n",
    "                obs = self.env.reset(input_item)\n",
    "                while True:\n",
    "                    action = self.agent.act(obs)\n",
    "                    obs, reward, done, pred = self.env.step(action)\n",
    "                    t += 1\n",
    "                    reset = t >= self.env.env_max_length\n",
    "                    self.agent.observe(obs, reward, done, reset)\n",
    "                    if done or reset:\n",
    "                        return pred.get('predicted_str')\n",
    "\n",
    "        # dataset = load_from_disk(self.input_dir)\n",
    "        # layer_list = cascade_ar_nar(self.ar_model, self.nar_model, self.ar_tokenizer, self.nar_tokenizer, dataset, self.device)\n",
    "        # encodec_code = convert_to_encode_code(self.nar_tokenizer, layer_list)    \n",
    "        # audio = synthesize_audio(encodec_code, self.device)\n",
    "        # output_path = f\"{self.output_dir}/{count}.wav\"\n",
    "        # sf.write(output_path, np.ravel(audio), samplerate=24000)\n",
    "\n",
    "    def agent_ppo(self, update_interval=10, minibatch_size=3000, epochs=20, lr=3e-6):\n",
    "        policy = torch.nn.Sequential(\n",
    "            self.middle_model,\n",
    "            self.remaining_model,\n",
    "            self.converter,\n",
    "            textrl.actor.SoftmaxCategoricalHead(self.env,\n",
    "                                   temperature=self.temperature,\n",
    "                                   top_k=self.top_k,\n",
    "                                   top_p=self.top_p)\n",
    "        )\n",
    "        vf = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.obs_size, self.obs_size // 2),\n",
    "            torch.nn.Linear(self.obs_size // 2, self.obs_size // 4),\n",
    "            torch.nn.Linear(self.obs_size // 4, 1)\n",
    "        )\n",
    "        model = pfrl.nn.Branched(policy, vf)\n",
    "        if isinstance(self.optimizer, str):\n",
    "            if self.optimizer.lower() == 'adamw':\n",
    "                opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "            else:\n",
    "                opt = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "        else:\n",
    "            opt = self.optimizer\n",
    "        model = model.cuda()\n",
    "        agent = VcPPOAgent(\n",
    "            model,\n",
    "            opt,\n",
    "            gpu=self.gpu_id,\n",
    "            update_interval=update_interval,\n",
    "            minibatch_size=minibatch_size,\n",
    "            epochs=epochs,\n",
    "            clip_eps_vf=None,\n",
    "            entropy_coef=0,\n",
    "            gamma=0.95,  # https://arxiv.org/abs/2210.01241\n",
    "            lambd=1,\n",
    "            max_grad_norm=1.0,\n",
    "            standardize_advantages=True,\n",
    "            act_deterministically=self.act_deterministically\n",
    "        )\n",
    "        self.agent = agent\n",
    "        return agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from encodec import EncodecModel\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from transformers import (AutoTokenizer, BartForConditionalGeneration,\n",
    "                          BatchEncoding)\n",
    "import vc.trainer_encodec_vc_inference as vc_inference\n",
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    dataset=\"lca0503/soxdata_small_encodec\",\n",
    "    splits=[\"train\"],\n",
    "    ground_truth_only=False,\n",
    "    cascade_ar_nar=True,\n",
    "    nar_model_only=False,\n",
    "    ground_truth_model_name=\"voidful/bart-base-unit\",\n",
    "    ar_checkpoint=\"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\",\n",
    "    nar_checkpoint=\"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\",\n",
    "    ground_truth_output_path=\"output_wav/vc/ground_truth/train_1.wav\",\n",
    "    cascade_output_path=\"output_wav/vc/ar_nar_cascade/train_1.wav\",\n",
    "    nar_output_path=\"output_wav/vc/nar/train_1.wav\",\n",
    "    seed=0,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "# modify ar_checkpoint in args\n",
    "args.ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets\n",
    "# from datasets import load_from_disk ,load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"lca0503/soxdata_encodec\")\n",
    "# dataset.save_to_disk(\"data\")\n",
    "\n",
    "# dataset = load_dataset(\"lca0503/soxdata_encodec\", split=\"+\".join([\"train\"]))\n",
    "# dataset = dataset.filter(lambda x : len(x[f\"src_encodec_0\"]) <= 700)\n",
    "# dataset = dataset.shuffle(0).select(range(1))\n",
    "\n",
    "# dataset.save_to_disk(\"data-encodec\")\n",
    "# dataset = load_from_disk(\"data-encodec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1. Agent to Environment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path\n",
    "agent_input_dir = f'{base_path}/data-encodec'\n",
    "agent_output_dir = f'{base_path}/output'\n",
    "env_input_dir = agent_output_dir\n",
    "env_output_dir = agent_input_dir\n",
    "\n",
    "ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "nar_checkpoint = \"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:  Play the audio twice.\n",
      "Transcription:  There is even a white row of beehives in the orchard, under the walnut trees.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "ar_model = BartForConditionalGeneration.from_pretrained(ar_checkpoint)\n",
    "ar_model.to(device)\n",
    "\n",
    "dataset = load_from_disk(agent_input_dir)\n",
    "instruction_ids = ar_tokenizer(dataset[\"instruction\"][0])[\"input_ids\"][1 : -1]\n",
    "transcription_ids = ar_tokenizer(dataset[\"transcription\"][0])[\"input_ids\"][1 : -1]\n",
    "instruction = dataset[\"instruction\"][0]\n",
    "transcription = dataset[\"transcription\"][0]\n",
    "\n",
    "print(\"Instruction: \", instruction)\n",
    "print(\"Transcription: \", transcription)\n",
    "\n",
    "# for i in range(len(instruction_ids)):\n",
    "#     print(\"Instruction(cascade): \", ar_tokenizer.decode(instruction_ids[i]))\n",
    "# for i in range(len(transcription_ids)):\n",
    "#     print(\"Transcription(cascade): \", ar_tokenizer.decode(transcription_ids[i]))\n",
    "    \n",
    "observation_list = [{'input': 0, 'transcription': transcription, 'instruction': instruction, 'dataset': dataset}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = VcRLEnv(model, tokenizer, observation_list, 100, 2, 1, env_input_dir, env_output_dir, instruction, transcription)\n",
    "# actor = VcActor(env, ar_model, ar_tokenizer, ar_checkpoint, nar_checkpoint, agent_input_dir, agent_output_dir, device, observation_list, 100, 2)\n",
    "# actor.predict(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e210ad103f0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# define env, actor, and agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# actor = TextRLActor(env,model,tokenizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVcRLEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_input_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_output_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranscription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVcActor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mar_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mar_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mar_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnar_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_input_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_output_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-bfa5b4d0ca67>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, observation_input, max_length, compare_sample, unfreeze_layer_from_past, env_input_dir, env_output_dir, instruction, transcription)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranscription\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_stop_toks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-bfa5b4d0ca67>\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, input_item)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_item\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-bfa5b4d0ca67>\u001b[0m in \u001b[0;36m_get_obs\u001b[0;34m(self, predicted)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'decoder'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                         feature_dict = self.tokenizer([self.gat_obs_input(self.input_item)],\n\u001b[0m\u001b[1;32m    103\u001b[0m                                                       \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                                                       \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2536\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2538\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2539\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_valid_text_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2596\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2597\u001b[0m                 \u001b[0;34m\"text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2598\u001b[0m                 \u001b[0;34m\"or `List[List[str]]` (batch of pretokenized examples).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "# define env, actor, and agent\n",
    "# actor = TextRLActor(env,model,tokenizer)\n",
    "env = VcRLEnv(model, tokenizer, observation_list, 100, 2, 1, env_input_dir, env_output_dir, instruction, transcription)\n",
    "actor = VcActor(env, ar_model, ar_tokenizer, ar_checkpoint, nar_checkpoint, agent_input_dir, agent_output_dir, device, observation_list, 100, 2)\n",
    "\n",
    "# for i in range(10):\n",
    "#     print(\"Step: \", i)\n",
    "#     actor.predict(i)\n",
    "#     env.step(i)\n",
    "#     actor.agent_ppo(update_interval=100, minibatch_size=3, epochs=10)\n",
    "    \n",
    "# agent = vc_inference.run(args, agent_input_dir, agent_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = actor.agent_ppo(update_interval=100, minibatch_size=3, epochs=10)\n",
    "pfrl.experiments.train_agent_with_evaluation(\n",
    "    agent,\n",
    "    env,\n",
    "    steps=300,\n",
    "    eval_n_steps=None,\n",
    "    eval_n_episodes=1,       \n",
    "    train_max_episode_len=100,  \n",
    "    eval_interval=10,\n",
    "    outdir='output', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor.predict(observaton_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2. Environment to Agent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward = env.get_reward()\n",
    "# print(\"reward\", reward)\n",
    "env._predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of agent (wav) + instruction + transcription"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
