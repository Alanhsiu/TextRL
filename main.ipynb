{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pfrl@git+https://github.com/voidful/pfrl.git\n",
    "# %pip install textrl==0.2.15\n",
    "# %pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(59481, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(59481, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(59481, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=59481, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\")\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pfrl\n",
    "from textrl import TextRLEnv,TextRLActor\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer, AutoModelWithLMHead\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "from NISQA.nisqa.NISQA_model import nisqaModel\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/work/b0990106x/TextRL'\n",
    "\n",
    "# print(\"model\", model)\n",
    "# print(\"tokenizer\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RL Environment: NISQA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.2.1 available.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import logging\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "from torch import autocast\n",
    "from vc.wav_to_arrow import process_audio\n",
    "\n",
    "class VcRLEnv(gym.Env):\n",
    "    def __init__(self, model, tokenizer, observation_input=[], max_length=100, compare_sample=2, unfreeze_layer_from_past=0, env_input_dir=None, env_output_dir=None, instruction=\"\", transcription=\"\"):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.observation_space = observation_input\n",
    "        self.compare_sample = compare_sample\n",
    "        self.unfreeze_layer_from_past = 1 if unfreeze_layer_from_past else 0\n",
    "        self.env_max_length = min(max(self.model.config.max_length, self.tokenizer.model_max_length), max_length)\n",
    "        self.env_input_dir = env_input_dir\n",
    "        self.env_output_dir = env_output_dir\n",
    "        self.instruction = instruction1\n",
    "        self.transcription = transcription\n",
    "        # self.reset()\n",
    "        \n",
    "        self.gen_stop_toks = []\n",
    "        logging.disable(sys.maxsize)\n",
    "        if self.tokenizer.sep_token:\n",
    "            self.gen_stop_toks.append(self.tokenizer.sep_token)\n",
    "        if self.tokenizer.eos_token:\n",
    "            self.gen_stop_toks.append(self.tokenizer.eos_token)\n",
    "        logging.disable(logging.NOTSET)\n",
    "    \n",
    "    @autocast('cuda')\n",
    "    def reset(self, input_item=None):\n",
    "        self.predicted = [[]] * self.compare_sample\n",
    "        self.predicted_end = [False] * self.compare_sample\n",
    "        self.input_item = {\"input\": \"\"}\n",
    "        return self._get_obs(self.predicted)\n",
    "\n",
    "    def step(self, count):\n",
    "        reward = self._predict(count)\n",
    "        self._get_obs(count)\n",
    "        return reward\n",
    "    \n",
    "    def get_reward(self, input_path=None, output_dir=None, count=0): # predicted will be the list of predicted token\n",
    "        args = {\n",
    "            'mode': 'predict_file', \n",
    "            'pretrained_model': f'{base_path}/NISQA/weights/nisqa.tar', \n",
    "            'deg': f'{base_path}/output/{count}.wav', \n",
    "            'data_dir': None, \n",
    "            'output_dir': f'{base_path}/NISQA/result',\n",
    "            'csv_file': None, \n",
    "            'csv_deg': None,  \n",
    "            'num_workers': 0, \n",
    "            'bs': 1,\n",
    "            'ms_channel': None\n",
    "        }\n",
    "\n",
    "        if input_path is not None:\n",
    "            args['deg'] = input_path\n",
    "\n",
    "        args['tr_bs_val'] = args['bs']\n",
    "        args['tr_num_workers'] = args['num_workers']\n",
    "        \n",
    "        nisqa = nisqaModel(args)\n",
    "        prediction = nisqa.predict()\n",
    "        reward = float(prediction['mos_pred'].iloc[0])\n",
    "        print(\"count\", count, \"reward\", reward)\n",
    "        return reward\n",
    "    \n",
    "        \n",
    "    def gat_obs_input(self, input_item):\n",
    "        return input_item['input']\n",
    "    \n",
    "    def _get_obs(self, count, predicted=[]):\n",
    "        # # backup\n",
    "        # audio_path = f\"{self.env_input_dir}/{count}.wav\"\n",
    "        # process_audio(source_audio_path=audio_path, output_dir = self.env_output_dir, temp_dir=\"/work/b0990106x/TextRL/vc/data/temp\", instruction=self.instruction, transcription=self.transcription)\n",
    "        # return count\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            obs_list = []\n",
    "            \n",
    "            for p_text in predicted:\n",
    "                p_text_str = self.tokenizer.convert_tokens_to_string(p_text)\n",
    "                if self.model.__class__.__name__ == 'OPTForCausalLM':\n",
    "                    feature_dict = self.tokenizer([[self.gat_obs_input(self.input_item), p_text_str]],\n",
    "                                                  return_tensors='pt',\n",
    "                                                  return_token_type_ids=False,\n",
    "                                                  add_special_tokens=False).to(self.model.device)\n",
    "                    with torch.cuda.amp.autocast(enabled=False):\n",
    "                        prediction = self.model(**feature_dict, output_hidden_states=True)\n",
    "                    outputs = prediction.hidden_states[-self.unfreeze_layer_from_past][:, -1, :]\n",
    "                else:\n",
    "                    if len([k for k, v in self.model.named_parameters() if 'decoder' in k]) > 0:\n",
    "                        feature_dict = self.tokenizer([self.gat_obs_input(self.input_item)],\n",
    "                                                      return_tensors='pt',\n",
    "                                                      return_token_type_ids=False,\n",
    "                                                      add_special_tokens=True).to(self.model.device)\n",
    "                        if len(p_text) > 0:\n",
    "                            decoder_input_ids = [self.model.config.decoder_start_token_id] + \\\n",
    "                                                self.tokenizer.convert_tokens_to_ids(p_text)\n",
    "                            dec_input = torch.tensor([decoder_input_ids]).to(self.model.device)\n",
    "                            feature_dict['decoder_input_ids'] = dec_input\n",
    "                        else:\n",
    "                            feature_dict['decoder_input_ids'] = torch.tensor(\n",
    "                                [[self.model.config.decoder_start_token_id]]).to(self.model.device)\n",
    "                        with torch.cuda.amp.autocast(enabled=False):\n",
    "                            prediction = self.model(**feature_dict, output_hidden_states=True)\n",
    "                        outputs = prediction.decoder_hidden_states[-self.unfreeze_layer_from_past].squeeze(0)\n",
    "                    else:\n",
    "                        if self.model.__class__.__name__ == 'DistributedBloomForCausalLM':\n",
    "                            with self.model.inference_session(max_length=self.env_max_length) as sess:\n",
    "                                feature_dict = self.tokenizer([[self.gat_obs_input(self.input_item), p_text_str]],\n",
    "                                                              return_tensors='pt',\n",
    "                                                              return_token_type_ids=False,\n",
    "                                                              add_special_tokens=False).to(self.model.device)\n",
    "                                embs = self.model.transformer.word_embeddings(feature_dict.input_ids)\n",
    "                                embs = self.model.transformer.word_embeddings_layernorm(embs)\n",
    "                                h = sess.step(embs)\n",
    "                                outputs = self.model.transformer.ln_f(h[:, -1])\n",
    "                        else:\n",
    "                            feature_dict = self.tokenizer([[self.gat_obs_input(self.input_item), p_text_str]],\n",
    "                                                          return_tensors='pt',\n",
    "                                                          return_token_type_ids=False,\n",
    "                                                          add_special_tokens=False).to(self.model.device)\n",
    "                            prediction = self.model(**feature_dict, output_hidden_states=True)\n",
    "                            outputs = prediction.hidden_states[-self.unfreeze_layer_from_past].squeeze(0)\n",
    "                obs_list.append(outputs.data[-1])\n",
    "            return (torch.stack(obs_list))\n",
    "        \n",
    "        \n",
    "    def _predict(self, count):\n",
    "        self.get_reward(count=count)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RL Agent: Text-Instruction-Guided Voice Conversion Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import autocast\n",
    "from transformers import (AutoTokenizer, BartForConditionalGeneration,\n",
    "                          BatchEncoding)\n",
    "from vc.trainer_encodec_vc_inference import cascade_ar_nar, convert_to_encode_code,synthesize_audio\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import textrl.actor\n",
    "\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pfrl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pfrl.agents.ppo import _elementwise_clip\n",
    "from pfrl.utils.mode_of_distribution import mode_of_distribution\n",
    "from torch import autocast\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "class VcPPOAgent(pfrl.agents.PPO):\n",
    "    def _update_if_dataset_is_ready(self):\n",
    "        dataset_size = (\n",
    "                sum(len(episode) for episode in self.memory)\n",
    "                + len(self.last_episode)\n",
    "                + (\n",
    "                    0\n",
    "                    if self.batch_last_episode is None\n",
    "                    else sum(len(episode) for episode in self.batch_last_episode)\n",
    "                )\n",
    "        )\n",
    "        if dataset_size >= self.update_interval:\n",
    "            self._flush_last_episode()\n",
    "            if self.recurrent:\n",
    "                dataset = pfrl.agents.ppo._make_dataset_recurrent(\n",
    "                    episodes=self.memory,\n",
    "                    model=self.model,\n",
    "                    phi=self.phi,\n",
    "                    batch_states=self.batch_states,\n",
    "                    obs_normalizer=self.obs_normalizer,\n",
    "                    gamma=self.gamma,\n",
    "                    lambd=self.lambd,\n",
    "                    max_recurrent_sequence_len=self.max_recurrent_sequence_len,\n",
    "                    device=self.device,\n",
    "                )\n",
    "                self._update_recurrent(dataset)\n",
    "            else:\n",
    "                dataset = pfrl.agents.ppo._make_dataset(\n",
    "                    episodes=self.memory,\n",
    "                    model=self.model,\n",
    "                    phi=self.phi,\n",
    "                    batch_states=self.batch_states,\n",
    "                    obs_normalizer=self.obs_normalizer,\n",
    "                    gamma=self.gamma,\n",
    "                    lambd=self.lambd,\n",
    "                    device=self.device,\n",
    "                )\n",
    "                assert len(dataset) == dataset_size\n",
    "                self._update(dataset)\n",
    "            self.explained_variance = self._compute_explained_variance(\n",
    "                list(itertools.chain.from_iterable(self.memory))\n",
    "            )\n",
    "            self.memory = []\n",
    "\n",
    "    def _compute_explained_variance(self, transitions):\n",
    "        \"\"\"Compute 1 - Var[return - v]/Var[return].\n",
    "\n",
    "        This function computes the fraction of variance that value predictions can\n",
    "        explain about returns.\n",
    "        \"\"\"\n",
    "        t = np.array([tr[\"v_teacher\"] for tr in transitions])\n",
    "        y = np.array([tr[\"v_pred\"] for tr in transitions])\n",
    "        vart = np.var(t)\n",
    "        if vart == 0:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return float(1 - np.var(np.average(t) - y) / vart)\n",
    "\n",
    "    def batch_act(self, batch_obs):\n",
    "        if self.training:\n",
    "            return self._batch_act_train(batch_obs)\n",
    "        else:\n",
    "            return self._batch_act_eval(batch_obs)\n",
    "\n",
    "    @autocast('cuda')\n",
    "    def _batch_act_eval(self, batch_obs):\n",
    "        assert not self.training\n",
    "        b_state = self.batch_states(batch_obs, self.device, self.phi)\n",
    "\n",
    "        if self.obs_normalizer:\n",
    "            b_state = self.obs_normalizer(b_state, update=False)\n",
    "\n",
    "        with torch.no_grad(), pfrl.utils.evaluating(self.model):\n",
    "            action_distrib, _ = self.model(b_state)\n",
    "            if self.act_deterministically:\n",
    "                action = mode_of_distribution(action_distrib).cpu().numpy()\n",
    "            else:\n",
    "                action = action_distrib.sample().cpu().numpy()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def _lossfun(\n",
    "            self, entropy, vs_pred, log_probs, vs_pred_old, log_probs_old, advs, vs_teacher\n",
    "    ):\n",
    "        prob_ratio = torch.exp(log_probs - log_probs_old)\n",
    "        loss_policy = -torch.mean(\n",
    "            torch.min(\n",
    "                (prob_ratio * advs),\n",
    "                torch.clamp(prob_ratio, 1 - self.clip_eps, 1 + self.clip_eps) * advs,\n",
    "            ),\n",
    "        )\n",
    "        if self.clip_eps_vf is None:\n",
    "            loss_value_func = F.mse_loss(vs_pred.squeeze(), vs_teacher.squeeze())\n",
    "        else:\n",
    "            clipped_vs_pred = _elementwise_clip(\n",
    "                vs_pred,\n",
    "                vs_pred_old - self.clip_eps_vf,\n",
    "                vs_pred_old + self.clip_eps_vf,\n",
    "            )\n",
    "            loss_value_func = torch.mean(\n",
    "                torch.max(\n",
    "                    F.mse_loss(vs_pred.squeeze(), vs_teacher, reduction=\"none\"),\n",
    "                    F.mse_loss(clipped_vs_pred.squeeze(), vs_teacher, reduction=\"none\"),\n",
    "                )\n",
    "            )\n",
    "        loss_entropy = -torch.mean(entropy)\n",
    "\n",
    "        self.value_loss_record.append(float(loss_value_func))\n",
    "        self.policy_loss_record.append(float(loss_policy))\n",
    "        loss = (\n",
    "                loss_policy\n",
    "                + self.value_func_coef * loss_value_func\n",
    "                + self.entropy_coef * loss_entropy\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "class VcActor():\n",
    "    def __init__(self, env, model, tokenizer,ar_checkpoint, nar_checkpoint, input_dir, output_dir, device, observation_input=[], max_length=100, compare_sample=2, gpu_id=0):\n",
    "        \n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.observation_input = observation_input\n",
    "        self.ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "        self.ar_model = BartForConditionalGeneration.from_pretrained(ar_checkpoint)\n",
    "        self.nar_tokenizer = AutoTokenizer.from_pretrained(nar_checkpoint)\n",
    "        self.nar_model = NARBartForConditionalGeneration.from_pretrained(nar_checkpoint)\n",
    "        self.gpu_id = gpu_id\n",
    "        # self.device = torch.device(device)\n",
    "        self.device = torch.device(\"cuda:{}\".format(gpu_id))\n",
    "        self.ar_model.to(self.device)\n",
    "        self.nar_model.to(self.device)\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    @autocast('cuda')\n",
    "    def predict(self, count):\n",
    "        t = 0 \n",
    "        with torch.inference_mode():\n",
    "            # use the model to predict the next wav\n",
    "            with self.agent.eval_mode():\n",
    "                obs = self.env.reset(self.observation_input)\n",
    "                while True:\n",
    "                    action = self.agent.act(obs)\n",
    "                    obs, reward, done, pred = self.env.step(action)\n",
    "                    t += 1\n",
    "                    reset = t >= self.env.env_max_length\n",
    "                    self.agent.observe(obs, reward, done, reset)\n",
    "                    if done or reset:\n",
    "                        return pred.get('predicted_str')\n",
    "\n",
    "        # dataset = load_from_disk(self.input_dir)\n",
    "        # layer_list = cascade_ar_nar(self.ar_model, self.nar_model, self.ar_tokenizer, self.nar_tokenizer, dataset, self.device)\n",
    "        # encodec_code = convert_to_encode_code(self.nar_tokenizer, layer_list)    \n",
    "        # audio = synthesize_audio(encodec_code, self.device)\n",
    "        # output_path = f\"{self.output_dir}/{count}.wav\"\n",
    "        # sf.write(output_path, np.ravel(audio), samplerate=24000)\n",
    "\n",
    "    def agent_ppo(self, update_interval=10, minibatch_size=3000, epochs=20, lr=3e-6):\n",
    "        policy = torch.nn.Sequential(\n",
    "            self.middle_model,\n",
    "            self.remaining_model,\n",
    "            self.converter,\n",
    "            textrl.actor.SoftmaxCategoricalHead(self.env,\n",
    "                                   temperature=self.temperature,\n",
    "                                   top_k=self.top_k,\n",
    "                                   top_p=self.top_p)\n",
    "        )\n",
    "        vf = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.obs_size, self.obs_size // 2),\n",
    "            torch.nn.Linear(self.obs_size // 2, self.obs_size // 4),\n",
    "            torch.nn.Linear(self.obs_size // 4, 1)\n",
    "        )\n",
    "        model = pfrl.nn.Branched(policy, vf)\n",
    "        if isinstance(self.optimizer, str):\n",
    "            if self.optimizer.lower() == 'adamw':\n",
    "                opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "            else:\n",
    "                opt = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "        else:\n",
    "            opt = self.optimizer\n",
    "        model = model.cuda()\n",
    "        agent = VcPPOAgent(\n",
    "            model,\n",
    "            opt,\n",
    "            gpu=self.gpu_id,\n",
    "            update_interval=update_interval,\n",
    "            minibatch_size=minibatch_size,\n",
    "            epochs=epochs,\n",
    "            clip_eps_vf=None,\n",
    "            entropy_coef=0,\n",
    "            gamma=0.95,  # https://arxiv.org/abs/2210.01241\n",
    "            lambd=1,\n",
    "            max_grad_norm=1.0,\n",
    "            standardize_advantages=True,\n",
    "            act_deterministically=self.act_deterministically\n",
    "        )\n",
    "        self.agent = agent\n",
    "        return agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from encodec import EncodecModel\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from transformers import (AutoTokenizer, BartForConditionalGeneration,\n",
    "                          BatchEncoding)\n",
    "import vc.trainer_encodec_vc_inference as vc_inference\n",
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    dataset=\"lca0503/soxdata_small_encodec\",\n",
    "    splits=[\"train\"],\n",
    "    ground_truth_only=False,\n",
    "    cascade_ar_nar=True,\n",
    "    nar_model_only=False,\n",
    "    ground_truth_model_name=\"voidful/bart-base-unit\",\n",
    "    ar_checkpoint=\"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\",\n",
    "    nar_checkpoint=\"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\",\n",
    "    ground_truth_output_path=\"output_wav/vc/ground_truth/train_1.wav\",\n",
    "    cascade_output_path=\"output_wav/vc/ar_nar_cascade/train_1.wav\",\n",
    "    nar_output_path=\"output_wav/vc/nar/train_1.wav\",\n",
    "    seed=0,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "# modify ar_checkpoint in args\n",
    "args.ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets\n",
    "# from datasets import load_from_disk ,load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"lca0503/soxdata_encodec\")\n",
    "# dataset.save_to_disk(\"data\")\n",
    "\n",
    "# dataset = load_dataset(\"lca0503/soxdata_encodec\", split=\"+\".join([\"train\"]))\n",
    "# dataset = dataset.filter(lambda x : len(x[f\"src_encodec_0\"]) <= 700)\n",
    "# dataset = dataset.shuffle(0).select(range(1))\n",
    "\n",
    "# dataset.save_to_disk(\"data-encodec\")\n",
    "# dataset = load_from_disk(\"data-encodec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1. Agent to Environment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path\n",
    "agent_input_dir = f'{base_path}/data-encodec'\n",
    "agent_output_dir = f'{base_path}/output'\n",
    "env_input_dir = agent_output_dir\n",
    "env_output_dir = agent_input_dir\n",
    "\n",
    "ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "nar_checkpoint = \"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:  Play the audio twice.\n",
      "Transcription:  There is even a white row of beehives in the orchard, under the walnut trees.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "ar_model = BartForConditionalGeneration.from_pretrained(ar_checkpoint)\n",
    "ar_model.to(device)\n",
    "\n",
    "dataset = load_from_disk(agent_input_dir)\n",
    "instruction_ids = ar_tokenizer(dataset[\"instruction\"][0])[\"input_ids\"][1 : -1]\n",
    "transcription_ids = ar_tokenizer(dataset[\"transcription\"][0])[\"input_ids\"][1 : -1]\n",
    "instruction = dataset[\"instruction\"][0]\n",
    "transcription = dataset[\"transcription\"][0]\n",
    "\n",
    "print(\"Instruction: \", instruction)\n",
    "print(\"Transcription: \", transcription)\n",
    "\n",
    "# for i in range(len(instruction_ids)):\n",
    "#     print(\"Instruction(cascade): \", ar_tokenizer.decode(instruction_ids[i]))\n",
    "# for i in range(len(transcription_ids)):\n",
    "#     print(\"Transcription(cascade): \", ar_tokenizer.decode(transcription_ids[i]))\n",
    "    \n",
    "observation_list = [{'input': 0, 'transcription': transcription, 'instruction': instruction, 'dataset': dataset}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Dataset({\n",
      "    features: ['file_id', 'instruction', 'transcription', 'src_encodec_0', 'src_encodec_1', 'src_encodec_2', 'src_encodec_3', 'src_encodec_4', 'src_encodec_5', 'src_encodec_6', 'src_encodec_7', 'tgt_encodec_0', 'tgt_encodec_1', 'tgt_encodec_2', 'tgt_encodec_3', 'tgt_encodec_4', 'tgt_encodec_5', 'tgt_encodec_6', 'tgt_encodec_7'],\n",
      "    num_rows: 4982\n",
      "})\n",
      "[20780, 5, 6086, 2330, 4]\n",
      "[970, 16, 190, 10, 1104, 3236, 9, 28, 14897, 3699, 11, 5, 50, 15782, 6, 223, 5, 21788, 10873, 3980, 4]\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "env = VcRLEnv(model, tokenizer, observation_list, 100, 2, 1, env_input_dir, env_output_dir, instruction, transcription)\n",
    "actor = VcActor(env, ar_model, ar_tokenizer, ar_checkpoint, nar_checkpoint, agent_input_dir, agent_output_dir, device, observation_list, 100, 2)\n",
    "actor.predict(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0\n",
      "Dataset Dataset({\n",
      "    features: ['file_id', 'instruction', 'transcription', 'src_encodec_0', 'src_encodec_1', 'src_encodec_2', 'src_encodec_3', 'src_encodec_4', 'src_encodec_5', 'src_encodec_6', 'src_encodec_7', 'tgt_encodec_0', 'tgt_encodec_1', 'tgt_encodec_2', 'tgt_encodec_3', 'tgt_encodec_4', 'tgt_encodec_5', 'tgt_encodec_6', 'tgt_encodec_7'],\n",
      "    num_rows: 4982\n",
      "})\n",
      "[20780, 5, 6086, 2330, 4]\n",
      "[970, 16, 190, 10, 1104, 3236, 9, 28, 14897, 3699, 11, 5, 50, 15782, 6, 223, 5, 21788, 10873, 3980, 4]\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Model architecture: NISQA_DIM\n",
      "Loaded pretrained model from /work/b0990106x/TextRL/NISQA/weights/nisqa.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages/librosa/filters.py:238: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 0 reward 2.0097849369049072\n",
      "[INFO] It took 0.11599326133728027 seconds to process the file.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006121158599853516,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Saving the dataset (0/1 shards)",
       "rate": null,
       "total": 1,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9f6cb2034a4ff5aa6679322603cf5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'VcActor' object has no attribute 'middle_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-aeda851d0b01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_ppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# agent = vc_inference.run(args, agent_input_dir, agent_output_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c511157a1e7b>\u001b[0m in \u001b[0;36magent_ppo\u001b[0;34m(self, update_interval, minibatch_size, epochs, lr)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magent_ppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         policy = torch.nn.Sequential(\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmiddle_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremaining_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VcActor' object has no attribute 'middle_model'"
     ]
    }
   ],
   "source": [
    "# define env, actor, and agent\n",
    "# actor = TextRLActor(env,model,tokenizer)\n",
    "env = VcRLEnv(model, tokenizer, observation_list, 100, 2, 1, env_input_dir, env_output_dir, instruction, transcription)\n",
    "actor = VcActor(env, ar_model, ar_tokenizer, ar_checkpoint, nar_checkpoint, agent_input_dir, agent_output_dir, device, observation_list, 100, 2)\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"Step: \", i)\n",
    "    actor.predict(i)\n",
    "    env.step(i)\n",
    "    actor.agent_ppo(update_interval=100, minibatch_size=3, epochs=10)\n",
    "    \n",
    "# agent = vc_inference.run(args, agent_input_dir, agent_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = actor.agent_ppo(update_interval=100, minibatch_size=3, epochs=10)\n",
    "pfrl.experiments.train_agent_with_evaluation(\n",
    "    agent,\n",
    "    env,\n",
    "    steps=300,\n",
    "    eval_n_steps=None,\n",
    "    eval_n_episodes=1,       \n",
    "    train_max_episode_len=100,  \n",
    "    eval_interval=10,\n",
    "    outdir='elon_musk_dogecoin', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor.predict(observaton_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2. Environment to Agent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward = env.get_reward()\n",
    "# print(\"reward\", reward)\n",
    "env._predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of agent (wav) + instruction + transcription"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
