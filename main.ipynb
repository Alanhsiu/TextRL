{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pfrl@git+https://github.com/voidful/pfrl.git\n",
    "# %pip install textrl==0.2.15\n",
    "# %pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(59481, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(59481, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(59481, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=59481, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\")\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do -> 5016\n",
      "Ġyou -> 47\n",
      "Ġknow -> 216\n",
      "Ġwhat -> 99\n",
      "Ġis -> 16\n",
      "Ġthe -> 5\n",
      "Ġcapital -> 812\n",
      "Ġof -> 9\n",
      "Ġfr -> 6664\n",
      "ance -> 2389\n",
      "? -> 116\n"
     ]
    }
   ],
   "source": [
    "# demo how the tokenization works\n",
    "tokens = tokenizer.tokenize(\"do you know what is the capital of france?\")\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "for tokens, token_id in zip(tokens, token_ids):\n",
    "    print(f\"{tokens} -> {token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pfrl\n",
    "from textrl import TextRLEnv,TextRLActor\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer, AutoModelWithLMHead\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "from NISQA.nisqa.NISQA_model import nisqaModel\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/work/b0990106x/TextRL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RL Environment: NISQA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.2.1 available.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import logging\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "from torch import autocast\n",
    "from vc.wav_to_arrow import process_audio\n",
    "\n",
    "class VcRLEnv(gym.Env):\n",
    "    def __init__(self, model, tokenizer, observation_input=[], max_length=100, compare_sample=2, unfreeze_layer_from_past=0, env_input_dir=None, env_output_dir=None, instruction=\"\", transcription=\"\"):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.observation_space = observation_input\n",
    "        self.compare_sample = compare_sample\n",
    "        self.unfreeze_layer_from_past = 1 if unfreeze_layer_from_past else 0\n",
    "        self.env_max_length = min(max(self.model.config.max_length, self.tokenizer.model_max_length), max_length)\n",
    "        self.env_input_dir = env_input_dir\n",
    "        self.env_output_dir = env_output_dir\n",
    "        self.instruction = instruction\n",
    "        self.transcription = transcription\n",
    "        self.reset()\n",
    "        \n",
    "        self.gen_stop_toks = []\n",
    "        logging.disable(sys.maxsize)\n",
    "        if self.tokenizer.sep_token:\n",
    "            self.gen_stop_toks.append(self.tokenizer.sep_token)\n",
    "        if self.tokenizer.eos_token:\n",
    "            self.gen_stop_toks.append(self.tokenizer.eos_token)\n",
    "        logging.disable(logging.NOTSET)\n",
    "    \n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        predicted, finish, predicted_str= self._predict(vocab_id=action)\n",
    "        reward = self.get_reward(self.input_item, predicted, finish)\n",
    "        self.predicted = predicted\n",
    "        return self._get_obs(predicted), reward, finish, {\"predicted_str\": predicted_str}\n",
    "\n",
    "    \n",
    "    def get_reward(self, input_path=None, output_dir=None, count=0): # predicted will be the list of predicted token\n",
    "        args = {\n",
    "            'mode': 'predict_file', \n",
    "            'pretrained_model': f'{base_path}/NISQA/weights/nisqa.tar', \n",
    "            'deg': f'{base_path}/output/{count}.wav', \n",
    "            'data_dir': None, \n",
    "            'output_dir': f'{base_path}/NISQA/result',\n",
    "            'csv_file': None, \n",
    "            'csv_deg': None,  \n",
    "            'num_workers': 0, \n",
    "            'bs': 1,\n",
    "            'ms_channel': None\n",
    "        }\n",
    "\n",
    "        if input_path is not None:\n",
    "            args['deg'] = input_path\n",
    "\n",
    "        args['tr_bs_val'] = args['bs']\n",
    "        args['tr_num_workers'] = args['num_workers']\n",
    "        \n",
    "        nisqa = nisqaModel(args)\n",
    "        prediction = nisqa.predict()\n",
    "        reward = float(prediction['mos_pred'].iloc[0])\n",
    "        print(\"count\", count, \"reward\", reward)\n",
    "        return reward\n",
    "    \n",
    "        \n",
    "    def gat_obs_input(self, input_item):\n",
    "        return input_item['input']\n",
    "    \n",
    "    @autocast('cuda')\n",
    "    def reset(self, input_item=None):\n",
    "        self.predicted = [[]] * self.compare_sample\n",
    "        self.predicted_end = [False] * self.compare_sample\n",
    "        self.input_item = {\"input\": \"\"}\n",
    "        if input_item is None:\n",
    "            self.input_item = random.choice(self.observation_space)\n",
    "        else:\n",
    "            self.input_item = input_item\n",
    "        return self._get_obs(self.predicted)\n",
    "    \n",
    "    @autocast('cuda')\n",
    "    def _get_obs(self, predicted=[]):\n",
    "        # # backup\n",
    "        # audio_path = f\"{self.env_input_dir}/{count}.wav\"\n",
    "        # process_audio(source_audio_path=audio_path, output_dir = self.env_output_dir, temp_dir=\"/work/b0990106x/TextRL/vc/data/temp\", instruction=self.instruction, transcription=self.transcription)\n",
    "        # return count\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            obs_list = []\n",
    "            for p_text in predicted:\n",
    "                p_text_str = self.tokenizer.convert_tokens_to_string(p_text)\n",
    "                if self.model.__class__.__name__ == 'OPTForCausalLM':\n",
    "                    feature_dict = self.tokenizer([[self.gat_obs_input(self.input_item), p_text_str]],\n",
    "                                                  return_tensors='pt',\n",
    "                                                  return_token_type_ids=False,\n",
    "                                                  add_special_tokens=False).to(self.model.device)\n",
    "                    with torch.cuda.amp.autocast(enabled=False):\n",
    "                        prediction = self.model(**feature_dict, output_hidden_states=True)\n",
    "                    outputs = prediction.hidden_states[-self.unfreeze_layer_from_past][:, -1, :]\n",
    "                else:\n",
    "                    if len([k for k, v in self.model.named_parameters() if 'decoder' in k]) > 0:\n",
    "                        feature_dict = self.tokenizer([self.gat_obs_input(self.input_item)],\n",
    "                                                      return_tensors='pt',\n",
    "                                                      return_token_type_ids=False,\n",
    "                                                      add_special_tokens=True).to(self.model.device)\n",
    "                        if len(p_text) > 0:\n",
    "                            decoder_input_ids = [self.model.config.decoder_start_token_id] + \\\n",
    "                                                self.tokenizer.convert_tokens_to_ids(p_text)\n",
    "                            dec_input = torch.tensor([decoder_input_ids]).to(self.model.device)\n",
    "                            feature_dict['decoder_input_ids'] = dec_input\n",
    "                        else:\n",
    "                            feature_dict['decoder_input_ids'] = torch.tensor(\n",
    "                                [[self.model.config.decoder_start_token_id]]).to(self.model.device)\n",
    "                        with torch.cuda.amp.autocast(enabled=False):\n",
    "                            prediction = self.model(**feature_dict, output_hidden_states=True)\n",
    "                        outputs = prediction.decoder_hidden_states[-self.unfreeze_layer_from_past].squeeze(0)\n",
    "                    else:\n",
    "                        if self.model.__class__.__name__ == 'DistributedBloomForCausalLM':\n",
    "                            with self.model.inference_session(max_length=self.env_max_length) as sess:\n",
    "                                feature_dict = self.tokenizer([[self.gat_obs_input(self.input_item), p_text_str]],\n",
    "                                                              return_tensors='pt',\n",
    "                                                              return_token_type_ids=False,\n",
    "                                                              add_special_tokens=False).to(self.model.device)\n",
    "                                embs = self.model.transformer.word_embeddings(feature_dict.input_ids)\n",
    "                                embs = self.model.transformer.word_embeddings_layernorm(embs)\n",
    "                                h = sess.step(embs)\n",
    "                                outputs = self.model.transformer.ln_f(h[:, -1])\n",
    "                        else:\n",
    "                            feature_dict = self.tokenizer([[self.gat_obs_input(self.input_item), p_text_str]],\n",
    "                                                          return_tensors='pt',\n",
    "                                                          return_token_type_ids=False,\n",
    "                                                          add_special_tokens=False).to(self.model.device)\n",
    "                            prediction = self.model(**feature_dict, output_hidden_states=True)\n",
    "                            outputs = prediction.hidden_states[-self.unfreeze_layer_from_past].squeeze(0)\n",
    "                obs_list.append(outputs.data[-1])\n",
    "            return (torch.stack(obs_list))\n",
    "        \n",
    "        \n",
    "    def _predict(self, vocab_id): \n",
    "        predicted_list = {}\n",
    "        predicted_list_end = {} \n",
    "        with torch.inference_mode():\n",
    "            for i, (v_id, predicted, predicted_end) in enumerate(zip(vocab_id, self.predicted, self.predicted_end)):\n",
    "                predicted_list_end[i] = False\n",
    "                if not predicted_end:\n",
    "                    pred_word = self.actions[v_id]\n",
    "                    if pred_word in self.gen_stop_toks \\\n",
    "                            or len(pred_word) < 1 \\\n",
    "                            or len(predicted) > self.env_max_length:\n",
    "                        predicted_list_end[i] = True\n",
    "                        predicted_list[i] = [pred_word]\n",
    "                    else:\n",
    "                        predicted_list[i] = [pred_word]\n",
    "                else:\n",
    "                    predicted_list_end[i] = True\n",
    "                    predicted_list[i] = ['']\n",
    "\n",
    "            for i, (l, e) in enumerate(zip(predicted_list.values(), predicted_list_end.values())):\n",
    "                self.predicted[i] = self.predicted[i] + l\n",
    "                self.predicted_end[i] = e\n",
    "\n",
    "            return self.predicted, all(self.predicted_end), [self.tokenizer.convert_tokens_to_string(i) for i in\n",
    "                                                             self.predicted]\n",
    "\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRLEnv(TextRLEnv):\n",
    "    # def get_reward(self, input_item, predicted_list, finish): # predicted will be the list of predicted token\n",
    "    #   reward = 0\n",
    "    #   if finish or len(predicted_list) >= self.env_max_length:\n",
    "    #     predicted_text = tokenizer.convert_tokens_to_string(predicted_list[0])\n",
    "    #     # sentiment classifier\n",
    "    #     reward = sentiment(input_item['input']+predicted_text)[0][0]['score'] * 10\n",
    "    #   return reward\n",
    "    \n",
    "    def get_reward(self, input_item, predicted_list, finish): # predicted will be the list of predicted token\n",
    "      count = 0\n",
    "      args = {\n",
    "          'mode': 'predict_file', \n",
    "          'pretrained_model': f'{base_path}/NISQA/weights/nisqa.tar', \n",
    "          'deg': f'{base_path}/output/{count}.wav', \n",
    "          'data_dir': None, \n",
    "          'output_dir': f'{base_path}/NISQA/result',\n",
    "          'csv_file': None, \n",
    "          'csv_deg': None,  \n",
    "          'num_workers': 0, \n",
    "          'bs': 1,\n",
    "          'ms_channel': None\n",
    "      }\n",
    "\n",
    "\n",
    "      args['tr_bs_val'] = args['bs']\n",
    "      args['tr_num_workers'] = args['num_workers']\n",
    "      \n",
    "      nisqa = nisqaModel(args)\n",
    "      prediction = nisqa.predict()\n",
    "      reward = float(prediction['mos_pred'].iloc[0])\n",
    "      print(\"count\", count, \"reward\", reward)\n",
    "      return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RL Agent: Text-Instruction-Guided Voice Conversion Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import autocast\n",
    "from transformers import (AutoTokenizer, BartForConditionalGeneration,\n",
    "                          BatchEncoding)\n",
    "from vc.trainer_encodec_vc_inference import cascade_ar_nar, convert_to_encode_code,synthesize_audio\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import textrl.actor\n",
    "\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pfrl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pfrl.agents.ppo import _elementwise_clip\n",
    "from pfrl.utils.mode_of_distribution import mode_of_distribution\n",
    "from torch import autocast\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "def get_modulelist_pos(model):\n",
    "    module_list_pos = 0\n",
    "    for ids, i in enumerate(list(model.children())):\n",
    "        if isinstance(i, torch.nn.ModuleList):\n",
    "            module_list_pos = ids\n",
    "    return module_list_pos\n",
    "\n",
    "\n",
    "class HFModelListModule(torch.nn.Module):\n",
    "    def __init__(self, module_list):\n",
    "        super(HFModelListModule, self).__init__()\n",
    "        self.module_list = module_list\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        for module in self.module_list:\n",
    "            hidden = module(hidden)[0]\n",
    "        return hidden\n",
    "\n",
    "class VcPPOAgent(pfrl.agents.PPO):\n",
    "    def _update_if_dataset_is_ready(self):\n",
    "        dataset_size = (\n",
    "                sum(len(episode) for episode in self.memory)\n",
    "                + len(self.last_episode)\n",
    "                + (\n",
    "                    0\n",
    "                    if self.batch_last_episode is None\n",
    "                    else sum(len(episode) for episode in self.batch_last_episode)\n",
    "                )\n",
    "        )\n",
    "        if dataset_size >= self.update_interval:\n",
    "            self._flush_last_episode()\n",
    "            if self.recurrent:\n",
    "                dataset = pfrl.agents.ppo._make_dataset_recurrent(\n",
    "                    episodes=self.memory,\n",
    "                    model=self.model,\n",
    "                    phi=self.phi,\n",
    "                    batch_states=self.batch_states,\n",
    "                    obs_normalizer=self.obs_normalizer,\n",
    "                    gamma=self.gamma,\n",
    "                    lambd=self.lambd,\n",
    "                    max_recurrent_sequence_len=self.max_recurrent_sequence_len,\n",
    "                    device=self.device,\n",
    "                )\n",
    "                self._update_recurrent(dataset)\n",
    "            else:\n",
    "                dataset = pfrl.agents.ppo._make_dataset(\n",
    "                    episodes=self.memory,\n",
    "                    model=self.model,\n",
    "                    phi=self.phi,\n",
    "                    batch_states=self.batch_states,\n",
    "                    obs_normalizer=self.obs_normalizer,\n",
    "                    gamma=self.gamma,\n",
    "                    lambd=self.lambd,\n",
    "                    device=self.device,\n",
    "                )\n",
    "                assert len(dataset) == dataset_size\n",
    "                self._update(dataset)\n",
    "            self.explained_variance = self._compute_explained_variance(\n",
    "                list(itertools.chain.from_iterable(self.memory))\n",
    "            )\n",
    "            self.memory = []\n",
    "\n",
    "    def _compute_explained_variance(self, transitions):\n",
    "        \"\"\"Compute 1 - Var[return - v]/Var[return].\n",
    "\n",
    "        This function computes the fraction of variance that value predictions can\n",
    "        explain about returns.\n",
    "        \"\"\"\n",
    "        t = np.array([tr[\"v_teacher\"] for tr in transitions])\n",
    "        y = np.array([tr[\"v_pred\"] for tr in transitions])\n",
    "        vart = np.var(t)\n",
    "        if vart == 0:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return float(1 - np.var(np.average(t) - y) / vart)\n",
    "\n",
    "    def batch_act(self, batch_obs):\n",
    "        if self.training:\n",
    "            return self._batch_act_train(batch_obs)\n",
    "        else:\n",
    "            return self._batch_act_eval(batch_obs)\n",
    "\n",
    "    @autocast('cuda')\n",
    "    def _batch_act_eval(self, batch_obs):\n",
    "        assert not self.training\n",
    "        b_state = self.batch_states(batch_obs, self.device, self.phi)\n",
    "\n",
    "        if self.obs_normalizer:\n",
    "            b_state = self.obs_normalizer(b_state, update=False)\n",
    "\n",
    "        with torch.no_grad(), pfrl.utils.evaluating(self.model):\n",
    "            action_distrib, _ = self.model(b_state)\n",
    "            if self.act_deterministically:\n",
    "                action = mode_of_distribution(action_distrib).cpu().numpy()\n",
    "            else:\n",
    "                action = action_distrib.sample().cpu().numpy()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def _lossfun(\n",
    "            self, entropy, vs_pred, log_probs, vs_pred_old, log_probs_old, advs, vs_teacher\n",
    "    ):\n",
    "        prob_ratio = torch.exp(log_probs - log_probs_old)\n",
    "        loss_policy = -torch.mean(\n",
    "            torch.min(\n",
    "                (prob_ratio * advs),\n",
    "                torch.clamp(prob_ratio, 1 - self.clip_eps, 1 + self.clip_eps) * advs,\n",
    "            ),\n",
    "        )\n",
    "        if self.clip_eps_vf is None:\n",
    "            loss_value_func = F.mse_loss(vs_pred.squeeze(), vs_teacher.squeeze())\n",
    "        else:\n",
    "            clipped_vs_pred = _elementwise_clip(\n",
    "                vs_pred,\n",
    "                vs_pred_old - self.clip_eps_vf,\n",
    "                vs_pred_old + self.clip_eps_vf,\n",
    "            )\n",
    "            loss_value_func = torch.mean(\n",
    "                torch.max(\n",
    "                    F.mse_loss(vs_pred.squeeze(), vs_teacher, reduction=\"none\"),\n",
    "                    F.mse_loss(clipped_vs_pred.squeeze(), vs_teacher, reduction=\"none\"),\n",
    "                )\n",
    "            )\n",
    "        loss_entropy = -torch.mean(entropy)\n",
    "\n",
    "        self.value_loss_record.append(float(loss_value_func))\n",
    "        self.policy_loss_record.append(float(loss_policy))\n",
    "        loss = (\n",
    "                loss_policy\n",
    "                + self.value_func_coef * loss_value_func\n",
    "                + self.entropy_coef * loss_entropy\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "class VcActor():\n",
    "    def __init__(self, env, model, tokenizer,ar_checkpoint, nar_checkpoint, input_dir, output_dir, device, observation_input=[], max_length=100, compare_sample=2, optimizer='sgd', gpu_id=0, unfreeze_layer_from_past=0,\n",
    "                 act_deterministically=True,\n",
    "                 temperature=1.0,\n",
    "                 top_k=0,\n",
    "                 top_p=1.0):\n",
    "        \n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.observation_input = observation_input\n",
    "        self.ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "        self.ar_model = BartForConditionalGeneration.from_pretrained(ar_checkpoint)\n",
    "        self.nar_tokenizer = AutoTokenizer.from_pretrained(nar_checkpoint)\n",
    "        self.nar_model = NARBartForConditionalGeneration.from_pretrained(nar_checkpoint)\n",
    "        self.gpu_id = gpu_id\n",
    "        # self.device = torch.device(device)\n",
    "        self.device = torch.device(\"cuda:{}\".format(gpu_id))\n",
    "        self.ar_model.to(self.device)\n",
    "        self.nar_model.to(self.device)\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # new added\n",
    "        self.act_deterministically = act_deterministically\n",
    "        self.temperature = temperature\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.optimizer = optimizer\n",
    "        self.unfreeze_layer_from_past = unfreeze_layer_from_past\n",
    "\n",
    "        parents = [parent[0] for parent in model.named_children()]\n",
    "        if 'transformer' in parents:  # gpt2/bloom:\n",
    "            transformers_model = model.transformer\n",
    "        elif 'model' in parents:  # bart\n",
    "            transformers_model = model.model\n",
    "        elif 'decoder' in parents:  # t5\n",
    "            transformers_model = model.decoder\n",
    "        else:\n",
    "            raise ValueError('model not supported')\n",
    "\n",
    "        if unfreeze_layer_from_past > 0:\n",
    "            self.middle_model = HFModelListModule(list(transformers_model.children())\n",
    "                                                  [get_modulelist_pos(transformers_model)]\n",
    "                                                  [-self.unfreeze_layer_from_past:])\n",
    "            self.remaining_model = torch.nn.Sequential(\n",
    "                *list(transformers_model.children())[get_modulelist_pos(transformers_model) + 1:])\n",
    "        else:\n",
    "            self.middle_model = torch.nn.Sequential()\n",
    "            self.remaining_model = torch.nn.Sequential()\n",
    "\n",
    "    @autocast('cuda')\n",
    "    def predict(self, input_item):\n",
    "        t = 0 \n",
    "        with torch.inference_mode():\n",
    "            # use the model to predict the next wav\n",
    "            with self.agent.eval_mode():\n",
    "                obs = self.env.reset(input_item)\n",
    "                while True:\n",
    "                    action = self.agent.act(obs)\n",
    "                    obs, reward, done, pred = self.env.step(action)\n",
    "                    t += 1\n",
    "                    reset = t >= self.env.env_max_length\n",
    "                    self.agent.observe(obs, reward, done, reset)\n",
    "                    if done or reset:\n",
    "                        return pred.get('predicted_str')\n",
    "\n",
    "        # dataset = load_from_disk(self.input_dir)\n",
    "        # layer_list = cascade_ar_nar(self.ar_model, self.nar_model, self.ar_tokenizer, self.nar_tokenizer, dataset, self.device)\n",
    "        # encodec_code = convert_to_encode_code(self.nar_tokenizer, layer_list)    \n",
    "        # audio = synthesize_audio(encodec_code, self.device)\n",
    "        # output_path = f\"{self.output_dir}/{count}.wav\"\n",
    "        # sf.write(output_path, np.ravel(audio), samplerate=24000)\n",
    "\n",
    "    def agent_ppo(self, update_interval=10, minibatch_size=3000, epochs=20, lr=3e-6):\n",
    "        policy = torch.nn.Sequential(\n",
    "            self.middle_model,\n",
    "            self.remaining_model,\n",
    "            self.converter,\n",
    "            textrl.actor.SoftmaxCategoricalHead(self.env,\n",
    "                                   temperature=self.temperature,\n",
    "                                   top_k=self.top_k,\n",
    "                                   top_p=self.top_p)\n",
    "        )\n",
    "        vf = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.obs_size, self.obs_size // 2),\n",
    "            torch.nn.Linear(self.obs_size // 2, self.obs_size // 4),\n",
    "            torch.nn.Linear(self.obs_size // 4, 1)\n",
    "        )\n",
    "        model = pfrl.nn.Branched(policy, vf)\n",
    "        if isinstance(self.optimizer, str):\n",
    "            if self.optimizer.lower() == 'adamw':\n",
    "                opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "            else:\n",
    "                opt = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "        else:\n",
    "            opt = self.optimizer\n",
    "        model = model.cuda()\n",
    "        agent = VcPPOAgent(\n",
    "            model,\n",
    "            opt,\n",
    "            gpu=self.gpu_id,\n",
    "            update_interval=update_interval,\n",
    "            minibatch_size=minibatch_size,\n",
    "            epochs=epochs,\n",
    "            clip_eps_vf=None,\n",
    "            entropy_coef=0,\n",
    "            gamma=0.95,  # https://arxiv.org/abs/2210.01241\n",
    "            lambd=1,\n",
    "            max_grad_norm=1.0,\n",
    "            standardize_advantages=True,\n",
    "            act_deterministically=self.act_deterministically\n",
    "        )\n",
    "        self.agent = agent\n",
    "        return agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from encodec import EncodecModel\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from transformers import (AutoTokenizer, BartForConditionalGeneration,\n",
    "                          BatchEncoding)\n",
    "import vc.trainer_encodec_vc_inference as vc_inference\n",
    "from types import SimpleNamespace\n",
    "\n",
    "a = SimpleNamespace(\n",
    "    dataset=\"lca0503/soxdata_small_encodec\",\n",
    "    splits=[\"train\"],\n",
    "    ground_truth_only=False,\n",
    "    cascade_ar_nar=True,\n",
    "    nar_model_only=False,\n",
    "    ground_truth_model_name=\"voidful/bart-base-unit\",\n",
    "    ar_checkpoint=\"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\",\n",
    "    nar_checkpoint=\"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\",\n",
    "    ground_truth_output_path=\"output_wav/vc/ground_truth/train_1.wav\",\n",
    "    cascade_output_path=\"output_wav/vc/ar_nar_cascade/train_1.wav\",\n",
    "    nar_output_path=\"output_wav/vc/nar/train_1.wav\",\n",
    "    seed=0,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "# modify ar_checkpoint in args\n",
    "args.ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets\n",
    "# from datasets import load_from_disk ,load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"lca0503/soxdata_encodec\")\n",
    "# dataset.save_to_disk(\"data\")\n",
    "\n",
    "# dataset = load_dataset(\"lca0503/soxdata_encodec\", split=\"+\".join([\"train\"]))\n",
    "# dataset = dataset.filter(lambda x : len(x[f\"src_encodec_0\"]) <= 700)\n",
    "# dataset = dataset.shuffle(0).select(range(1))\n",
    "\n",
    "# dataset.save_to_disk(\"data-encodec\")\n",
    "# dataset = load_from_disk(\"data-encodec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "observaton_list = [{'input':'i think dogecoin is'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "damn!\n",
      "obs_list:  [tensor([ 2.1733e-02,  2.3512e+00,  1.8789e+00, -6.8462e-01,  1.0484e+00,\n",
      "         1.3062e+00,  9.7844e-01,  1.6552e-01, -3.8541e-01,  7.5027e-01,\n",
      "         1.1449e+00, -1.6185e-01, -7.8862e-01, -1.5204e-01,  1.2085e+00,\n",
      "         4.2193e-01, -3.5139e-01,  1.0533e+00, -1.5065e+00, -8.3882e-01,\n",
      "        -5.4748e-01, -2.4958e-01, -4.3012e-01,  6.9651e-01, -3.2059e-01,\n",
      "         1.5985e+00,  1.6794e+00, -3.5632e+01,  3.3386e+00, -4.2510e+00,\n",
      "        -2.0316e-01, -1.0961e+00, -3.4016e-01, -1.7818e+00,  1.2341e+00,\n",
      "         1.3520e+00, -1.1822e+00,  6.6490e-01, -7.4144e-02,  1.2108e-02,\n",
      "        -1.6391e+00,  6.0387e-01,  3.1831e-01, -3.2095e-01, -1.0343e+00,\n",
      "         6.1949e-01,  4.8022e-01, -3.1972e-01, -9.7182e-01, -4.8905e-01,\n",
      "         2.6329e-01, -7.8475e-01,  2.1707e-01, -2.6556e+00,  6.3245e-01,\n",
      "         1.6951e+00,  1.4185e+00,  1.2172e+00, -7.7984e-01, -1.6424e+00,\n",
      "        -5.4762e-01, -1.2510e+00,  1.3047e+00,  4.8464e-01, -1.1859e+00,\n",
      "         6.0458e-01, -1.1320e+00, -9.8938e-01,  1.0340e+00,  9.9348e-01,\n",
      "         1.3053e+00, -4.0913e-01, -1.1167e+00,  2.3824e-01,  1.3064e+00,\n",
      "        -4.3951e-01, -1.5451e+00,  9.5704e-02, -3.7370e-01,  5.5943e-01,\n",
      "        -1.7317e+00,  3.8155e-01, -8.4177e-01,  9.0739e-01,  1.6162e-01,\n",
      "         1.4737e+00,  1.0076e+00,  3.6307e-01, -7.1370e-02, -6.9325e-01,\n",
      "         7.7696e-01,  1.8704e+00,  5.3235e-01,  1.0610e+00,  7.8837e-01,\n",
      "        -1.0441e-01, -1.5124e+00,  9.0291e-01, -3.9880e-01,  5.9890e-01,\n",
      "        -6.6520e-01, -1.1427e-01,  1.9612e-01,  1.1127e-01,  5.1839e-01,\n",
      "        -5.7522e-01,  5.1916e-01,  1.6439e-01, -2.1524e-01, -9.6926e-01,\n",
      "         8.2584e-01,  2.1601e+00, -2.8292e-01, -9.1493e-01,  6.1228e-01,\n",
      "         5.4185e-01,  2.0156e-01, -1.0026e+00, -4.8491e-01, -4.1908e-01,\n",
      "         9.9127e-01,  1.7914e-01,  8.3401e-01,  1.0541e+00,  5.8926e-01,\n",
      "         2.9630e-01, -3.4728e+00,  9.7247e-01, -2.3479e-01, -1.0634e-01,\n",
      "        -1.1870e+00,  4.3611e-01,  1.1273e+00, -2.3157e-01, -1.3193e+00,\n",
      "         7.4986e-02,  1.1531e+00,  1.7918e+00,  1.3271e-01,  3.1489e-01,\n",
      "         1.3064e-01,  2.2416e-01,  2.2780e-01,  1.8197e-01, -3.1231e-01,\n",
      "         2.4287e-01,  9.1771e-02,  1.2171e+00,  2.3574e-01,  2.6137e+00,\n",
      "        -4.8375e-01, -1.3773e-01, -8.0536e-01, -6.8390e-01, -1.3580e-01,\n",
      "        -7.2321e-02, -1.2005e+00,  7.8990e-01,  1.1074e+00, -2.7828e-01,\n",
      "         1.8993e-01,  3.4145e-01, -4.8253e-02,  3.3586e-01,  7.7154e-02,\n",
      "        -4.6405e-01,  7.0414e-01,  9.7096e-01, -1.1120e+00,  1.0051e+00,\n",
      "         3.0286e+00,  4.3220e-01, -8.9888e-01,  9.3815e-01, -1.1919e+00,\n",
      "         2.7993e-02,  1.2970e+00, -9.2049e-01,  1.3638e-01, -1.3140e+00,\n",
      "        -6.5508e-01, -7.2400e-01,  3.8801e-01, -1.7226e-01,  6.5055e-01,\n",
      "        -4.3494e-01,  7.8169e-01,  1.0111e+00,  1.2543e+00,  9.1810e-01,\n",
      "         9.8878e-01,  9.6846e-01, -2.4586e-01, -1.2371e+00,  1.1517e-01,\n",
      "         8.5569e-01, -1.6835e+00,  8.2980e-01,  1.2310e+00,  1.6650e+00,\n",
      "        -1.6553e-01,  4.5191e-01, -3.3498e-01,  7.7962e-01,  2.3447e-01,\n",
      "         4.5123e-02,  3.3153e+00,  1.5865e+00,  9.4514e-01, -4.2203e-02,\n",
      "         8.7318e-01, -9.7298e-01,  1.4437e+00, -6.7379e-01,  1.1175e-01,\n",
      "         2.1518e-01, -5.7973e-01,  1.0289e+00,  4.2738e-01, -3.4263e-02,\n",
      "         1.6757e-01, -3.0877e-02,  6.7602e-01,  7.9766e-01, -9.0044e-01,\n",
      "         1.4708e-01,  1.8366e+00,  8.0352e-01,  2.7394e-01,  2.3090e-01,\n",
      "        -8.6506e-01, -3.6357e-01, -2.2211e-01,  7.4435e-01, -4.8689e-01,\n",
      "         2.5621e-01, -1.5109e+00,  2.0811e+00, -2.6663e-01,  4.5646e-01,\n",
      "        -4.4062e-01, -1.7446e-01,  1.6396e+00,  3.6986e-01, -5.3908e-01,\n",
      "        -1.8466e-01, -8.8921e-01, -1.0976e+00,  8.8283e-01,  7.4406e-01,\n",
      "        -2.4945e-01,  2.5362e+00, -1.4620e+00,  2.1674e+00,  1.1511e-01,\n",
      "        -1.8325e-01,  4.2442e-01, -1.2023e-01, -1.1702e+00, -4.3101e-02,\n",
      "         1.2681e-01, -1.0813e+00,  2.4871e+00,  2.9440e-01,  3.9919e-01,\n",
      "        -2.8415e-01,  8.7073e-01,  1.5537e+00, -1.3806e-01,  1.1208e+00,\n",
      "         2.1753e+00,  1.0261e+00, -7.0560e-02,  1.2369e+00, -2.8864e-01,\n",
      "         3.5906e-01, -4.8617e-01, -6.7251e-01,  7.0562e-01, -7.2866e-01,\n",
      "         1.8239e+00,  2.5639e-01,  3.1876e-01, -8.8112e-01, -5.9453e-01,\n",
      "         1.7659e+00,  3.8116e-01, -4.2477e-01,  1.4635e+00,  8.2531e-01,\n",
      "        -7.7232e-02,  5.0972e-01,  7.6466e-01,  1.1482e+00,  4.7100e-01,\n",
      "         2.7478e-01,  2.6568e+00,  1.6093e-01,  8.1569e-01, -1.1459e+00,\n",
      "        -1.5671e+00,  1.2660e+00, -8.2909e-01,  5.5378e-01,  2.9655e-01,\n",
      "         6.3990e-01,  8.4689e-01, -7.6991e-01, -5.5168e-01,  1.8279e-01,\n",
      "         1.4295e-01, -7.9998e-01,  8.0112e-01, -7.3067e-01,  1.2755e-01,\n",
      "         4.5089e-01,  3.1558e-01,  1.1079e+00, -2.3889e-01,  7.0280e-01,\n",
      "         1.1891e+00,  1.8824e+00, -1.3028e+00,  1.0746e-01, -2.1063e+00,\n",
      "         7.9045e-01, -1.0622e+00,  2.3672e-01,  7.2063e-02,  4.1157e-01,\n",
      "        -7.9173e-02,  6.5617e-01,  6.7676e-01,  4.9162e-01,  2.6651e-01,\n",
      "        -4.4940e-01,  1.3236e+00, -6.1010e-01, -1.1714e-01, -1.0956e+00,\n",
      "        -2.5211e-01,  2.6412e-01, -5.6244e-01, -2.0128e-01, -8.4071e-01,\n",
      "         3.2648e-01, -2.9524e-02,  4.7261e-01, -7.0385e-01, -4.5095e-01,\n",
      "        -1.6404e+00,  6.9296e-01,  1.5616e+00, -2.1614e+00,  2.1413e+00,\n",
      "         1.3553e+00, -5.9899e-01, -5.4535e-01,  1.1470e+00, -5.9674e-01,\n",
      "         1.1943e-01,  8.1192e-01,  4.9309e-01,  9.1780e-01,  9.7311e-01,\n",
      "        -2.3779e-01, -2.1455e-01, -2.0275e-01, -1.9906e-01, -2.5821e-01,\n",
      "        -1.2128e-01,  2.8065e-02,  8.3774e-01, -1.6566e+00, -1.2044e+00,\n",
      "        -4.6246e-01,  3.2241e-01, -4.7732e-01,  8.5060e-01,  1.2728e+00,\n",
      "         6.1689e-01, -1.6019e-01, -1.4170e+00, -7.3691e-01,  2.8800e-02,\n",
      "        -2.0656e+00,  4.5778e-01, -1.0017e+00, -4.4263e-01,  7.5992e-01,\n",
      "        -4.0019e-01, -7.0172e-01, -1.9934e-01,  9.4654e-01,  5.1082e-01,\n",
      "        -8.8172e-01,  1.9634e+00, -1.7040e-02, -9.7940e-01, -2.5060e-01,\n",
      "         2.3286e+00, -6.0682e-01, -9.1782e-02,  9.7688e-01,  2.1971e+00,\n",
      "         2.6314e-01,  6.4284e-01,  9.5231e-01,  7.3963e-01,  2.4993e-01,\n",
      "        -3.5515e-01,  4.0766e-01, -6.9184e-01,  2.0599e-01, -5.0592e-02,\n",
      "        -2.8633e-01,  6.3983e-01,  1.2470e-01,  3.3598e-01, -7.4460e-01,\n",
      "        -4.1632e-01,  1.1031e-01,  6.8842e-01,  9.1074e-01,  4.0774e-01,\n",
      "         5.1861e-01,  1.4586e-01,  6.5514e-01,  1.0497e+00,  1.0873e-01,\n",
      "        -2.5422e-01, -3.5213e+00,  2.5822e-01,  2.8830e-01,  1.5232e+00,\n",
      "        -3.5837e-01, -9.2339e-01,  1.0798e+00,  8.2003e-01,  4.5518e-01,\n",
      "         1.3512e+00, -4.5006e-01,  1.1941e+00, -9.2219e-01, -3.5163e-01,\n",
      "        -5.6588e-01, -6.3999e-04,  4.7565e-01, -1.1391e+00,  1.6159e+00,\n",
      "        -1.5083e+00, -2.0313e-01, -2.2908e-02, -9.2437e-01, -7.7440e-01,\n",
      "        -1.2014e-01, -2.6423e-01,  2.3239e-01,  8.5398e-01,  9.6474e-01,\n",
      "        -4.7080e-01, -2.6546e-01,  4.4321e-01,  7.5205e-01, -9.5050e-01,\n",
      "        -1.4183e+00,  1.3610e+00,  4.9172e-01, -8.8746e-01,  5.6487e-01,\n",
      "         1.6774e+00,  2.0109e-01, -7.4961e-01, -5.2640e-01,  1.8136e+00,\n",
      "         1.4779e+00,  1.0931e+00, -7.0960e-01,  8.8124e-01,  1.2791e+00,\n",
      "        -9.0092e-01,  3.5434e-01,  1.4757e+00,  1.0617e+00, -1.1026e+00,\n",
      "         3.1254e-02, -7.1938e-01,  1.9181e-01, -5.9484e-01, -5.0550e-02,\n",
      "        -4.0379e-01, -1.2001e+00, -5.5530e-01,  9.3769e-02,  4.0426e-01,\n",
      "         5.7039e-01, -1.7078e+00, -5.5696e+00,  9.6597e-01, -1.5975e+00,\n",
      "        -5.0947e-01,  1.2915e+00, -1.2400e+00,  8.4634e-01, -2.4607e-01,\n",
      "        -5.8577e-01,  1.2375e+00, -1.0058e+00,  1.8627e-01,  2.0600e-01,\n",
      "         8.4824e-01, -1.0523e+00,  7.4133e-01, -1.1318e+00,  9.1530e-01,\n",
      "         8.2814e-01,  1.1389e+00,  7.1528e-01,  2.1518e+00,  3.6182e-01,\n",
      "        -1.2914e-02,  3.7558e-01, -1.7550e-01, -2.9095e-01,  4.2451e-01,\n",
      "        -4.4926e-01,  1.1149e+00,  4.5106e-01,  1.2188e+00, -1.1361e+00,\n",
      "         1.5387e-01, -8.3496e-02, -1.4450e+00,  5.9026e-01,  1.5560e+00,\n",
      "        -2.4388e-01, -2.6984e-01, -1.4401e-01,  1.8707e+00,  1.5285e+00,\n",
      "        -1.1984e+00,  1.0201e+00, -5.7524e-01, -1.1946e-01,  1.4504e+00,\n",
      "         1.2215e+00,  1.5139e-01, -2.0167e+00,  4.0510e-01,  1.4929e+00,\n",
      "        -5.4596e-01, -8.4858e-01,  1.2047e+00, -7.4683e-02, -1.7684e+00,\n",
      "         4.7179e-01,  8.2556e-01,  1.7216e+00, -6.1923e-01, -3.9332e-01,\n",
      "         6.6955e-01, -1.0773e-01,  2.2362e-01, -2.1947e-03,  4.2518e-01,\n",
      "        -8.1911e-01,  7.0022e-01,  1.4824e+00, -1.1430e+00,  9.8547e-01,\n",
      "        -2.3934e-01,  8.4676e-01,  1.0346e+00,  1.3161e+00,  2.3286e+00,\n",
      "        -9.3676e-02, -4.4837e-01, -1.4270e+00,  8.3490e-01, -9.1071e-01,\n",
      "         1.7665e+00,  1.1520e+00, -1.3079e+00,  1.5837e-01,  6.5319e-03,\n",
      "         1.3581e+00,  1.6953e+00, -1.1441e+00,  1.0625e+00,  7.9216e-01,\n",
      "        -5.6555e-02,  2.2631e+00, -3.9288e-01, -9.4371e-01,  3.6694e-01,\n",
      "        -4.3764e-01,  8.9768e-01,  8.0142e-01,  5.7932e-01,  1.0645e+00,\n",
      "        -1.1275e+00,  1.2109e+00,  3.3829e-01, -3.0146e+00, -3.5758e-02,\n",
      "         2.8762e-01, -1.3387e+00,  8.6288e-01, -1.8525e+00, -8.8965e-01,\n",
      "         7.8217e-01,  1.0310e+00, -7.6667e-01,  6.5482e-01, -3.0523e-01,\n",
      "        -3.6386e-01, -4.1549e-01,  5.1824e-01,  4.5465e-01, -1.2476e+00,\n",
      "         1.7196e+00,  1.2815e+00,  1.5388e-01,  1.6567e+00,  5.6966e-01,\n",
      "         5.8139e-01,  2.1303e+00, -7.1467e-01,  5.1486e-01, -9.1936e-01,\n",
      "        -2.7004e-01, -2.0197e-01, -1.9870e-01, -1.2895e+00, -2.0033e+00,\n",
      "         8.5783e-01, -1.4502e-01, -8.9922e-02, -7.2666e-01, -3.4466e-01,\n",
      "        -3.9614e-01,  1.5499e+00,  3.4891e-01,  1.0848e+00, -2.9357e-02,\n",
      "        -5.0754e-01,  1.3576e+00,  1.2169e+00, -1.4366e-01, -5.0489e-01,\n",
      "        -2.1302e-01,  4.5974e-01, -2.7319e+00, -2.7049e-01, -3.7016e-01,\n",
      "        -2.8187e-01, -2.4903e-01, -1.4284e+00, -6.4017e-01, -8.0612e-01,\n",
      "        -9.1343e-01, -2.2223e-01,  3.3007e-01, -3.7590e-01, -9.1547e-01,\n",
      "        -8.6983e-02,  1.4000e+00,  1.3854e+00,  5.3940e-01,  5.8141e-01,\n",
      "         5.0860e-02, -3.4714e-01, -4.7421e-01, -1.6035e+00,  4.1397e-01,\n",
      "        -1.2125e+00,  4.0013e-01,  2.4710e-01, -3.4936e-01, -2.0202e-01,\n",
      "         6.3854e-01, -1.1483e+00, -9.3708e-01,  5.8992e-01,  1.0569e+00,\n",
      "         3.2500e-01,  5.0334e-01,  1.0579e+00,  1.3065e+00,  1.0260e+00,\n",
      "        -5.2911e-01, -3.6356e+00, -9.2289e-01,  9.6436e-01,  9.0955e-01,\n",
      "         8.8819e-01, -7.9956e-02,  3.0763e-01, -8.2040e-01,  5.9451e-01,\n",
      "         1.0903e+00, -1.0123e+00, -1.6947e+00,  1.4287e+00,  4.5450e-01,\n",
      "        -1.2434e+00, -1.2724e+00, -3.0088e+00,  6.7408e-01,  1.5204e-01,\n",
      "         5.9779e-02,  1.0458e+00,  2.1062e-01,  2.1883e-01, -2.5954e+00,\n",
      "        -3.0023e-01, -5.5525e-01,  1.8862e-02,  1.3797e+00, -6.7477e-01,\n",
      "         1.8881e+00,  1.0713e+00,  1.0710e+00, -1.2661e+00,  8.6746e-02,\n",
      "        -4.7249e-01,  1.0027e+00, -3.1733e-01,  7.6788e-01,  2.8536e-01,\n",
      "        -1.8975e+00,  3.0508e-01,  1.9590e-01, -3.5925e-01, -4.5329e-01,\n",
      "         7.5210e-01,  6.2905e+00,  2.1742e+00,  5.1069e-02, -1.0146e+00,\n",
      "         3.6670e-01,  1.1211e+00,  1.3807e-02, -1.5196e-01,  6.6334e-01,\n",
      "         7.7211e-01, -1.8399e+00,  2.1950e+00, -9.9795e-01, -1.6650e-01,\n",
      "        -9.3371e-01, -1.1487e+00, -6.0442e-01, -3.8167e-01, -6.3974e-01,\n",
      "         1.8929e+00, -3.4243e-01,  3.6730e-01,  4.6468e-01,  2.4817e-01,\n",
      "        -7.3952e-01,  8.8391e-01,  3.9626e-01, -1.5908e+00, -3.5482e-02,\n",
      "        -1.0931e-01,  5.7332e-02,  9.3566e-01], device='cuda:0')]\n",
      "feature_dict:  {'input_ids': tensor([[    0,   118,   206,   109,  1899, 16973,    16,     2]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'decoder_input_ids': tensor([[2]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "env = MyRLEnv(model, tokenizer, observation_input=observaton_list,compare_sample=1)\n",
    "actor = TextRLActor(env,model,tokenizer)\n",
    "agent = actor.agent_ppo(update_interval=100, minibatch_size=3, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_list:  [tensor([ 2.1733e-02,  2.3512e+00,  1.8789e+00, -6.8462e-01,  1.0484e+00,\n",
      "         1.3062e+00,  9.7844e-01,  1.6552e-01, -3.8541e-01,  7.5027e-01,\n",
      "         1.1449e+00, -1.6185e-01, -7.8862e-01, -1.5204e-01,  1.2085e+00,\n",
      "         4.2193e-01, -3.5139e-01,  1.0533e+00, -1.5065e+00, -8.3882e-01,\n",
      "        -5.4748e-01, -2.4958e-01, -4.3012e-01,  6.9651e-01, -3.2059e-01,\n",
      "         1.5985e+00,  1.6794e+00, -3.5632e+01,  3.3386e+00, -4.2510e+00,\n",
      "        -2.0316e-01, -1.0961e+00, -3.4016e-01, -1.7818e+00,  1.2341e+00,\n",
      "         1.3520e+00, -1.1822e+00,  6.6490e-01, -7.4144e-02,  1.2108e-02,\n",
      "        -1.6391e+00,  6.0387e-01,  3.1831e-01, -3.2095e-01, -1.0343e+00,\n",
      "         6.1949e-01,  4.8022e-01, -3.1972e-01, -9.7182e-01, -4.8905e-01,\n",
      "         2.6329e-01, -7.8475e-01,  2.1707e-01, -2.6556e+00,  6.3245e-01,\n",
      "         1.6951e+00,  1.4185e+00,  1.2172e+00, -7.7984e-01, -1.6424e+00,\n",
      "        -5.4762e-01, -1.2510e+00,  1.3047e+00,  4.8464e-01, -1.1859e+00,\n",
      "         6.0458e-01, -1.1320e+00, -9.8938e-01,  1.0340e+00,  9.9348e-01,\n",
      "         1.3053e+00, -4.0913e-01, -1.1167e+00,  2.3824e-01,  1.3064e+00,\n",
      "        -4.3951e-01, -1.5451e+00,  9.5704e-02, -3.7370e-01,  5.5943e-01,\n",
      "        -1.7317e+00,  3.8155e-01, -8.4177e-01,  9.0739e-01,  1.6162e-01,\n",
      "         1.4737e+00,  1.0076e+00,  3.6307e-01, -7.1370e-02, -6.9325e-01,\n",
      "         7.7696e-01,  1.8704e+00,  5.3235e-01,  1.0610e+00,  7.8837e-01,\n",
      "        -1.0441e-01, -1.5124e+00,  9.0291e-01, -3.9880e-01,  5.9890e-01,\n",
      "        -6.6520e-01, -1.1427e-01,  1.9612e-01,  1.1127e-01,  5.1839e-01,\n",
      "        -5.7522e-01,  5.1916e-01,  1.6439e-01, -2.1524e-01, -9.6926e-01,\n",
      "         8.2584e-01,  2.1601e+00, -2.8292e-01, -9.1493e-01,  6.1228e-01,\n",
      "         5.4185e-01,  2.0156e-01, -1.0026e+00, -4.8491e-01, -4.1908e-01,\n",
      "         9.9127e-01,  1.7914e-01,  8.3401e-01,  1.0541e+00,  5.8926e-01,\n",
      "         2.9630e-01, -3.4728e+00,  9.7247e-01, -2.3479e-01, -1.0634e-01,\n",
      "        -1.1870e+00,  4.3611e-01,  1.1273e+00, -2.3157e-01, -1.3193e+00,\n",
      "         7.4986e-02,  1.1531e+00,  1.7918e+00,  1.3271e-01,  3.1489e-01,\n",
      "         1.3064e-01,  2.2416e-01,  2.2780e-01,  1.8197e-01, -3.1231e-01,\n",
      "         2.4287e-01,  9.1771e-02,  1.2171e+00,  2.3574e-01,  2.6137e+00,\n",
      "        -4.8375e-01, -1.3773e-01, -8.0536e-01, -6.8390e-01, -1.3580e-01,\n",
      "        -7.2321e-02, -1.2005e+00,  7.8990e-01,  1.1074e+00, -2.7828e-01,\n",
      "         1.8993e-01,  3.4145e-01, -4.8253e-02,  3.3586e-01,  7.7154e-02,\n",
      "        -4.6405e-01,  7.0414e-01,  9.7096e-01, -1.1120e+00,  1.0051e+00,\n",
      "         3.0286e+00,  4.3220e-01, -8.9888e-01,  9.3815e-01, -1.1919e+00,\n",
      "         2.7993e-02,  1.2970e+00, -9.2049e-01,  1.3638e-01, -1.3140e+00,\n",
      "        -6.5508e-01, -7.2400e-01,  3.8801e-01, -1.7226e-01,  6.5055e-01,\n",
      "        -4.3494e-01,  7.8169e-01,  1.0111e+00,  1.2543e+00,  9.1810e-01,\n",
      "         9.8878e-01,  9.6846e-01, -2.4586e-01, -1.2371e+00,  1.1517e-01,\n",
      "         8.5569e-01, -1.6835e+00,  8.2980e-01,  1.2310e+00,  1.6650e+00,\n",
      "        -1.6553e-01,  4.5191e-01, -3.3498e-01,  7.7962e-01,  2.3447e-01,\n",
      "         4.5123e-02,  3.3153e+00,  1.5865e+00,  9.4514e-01, -4.2203e-02,\n",
      "         8.7318e-01, -9.7298e-01,  1.4437e+00, -6.7379e-01,  1.1175e-01,\n",
      "         2.1518e-01, -5.7973e-01,  1.0289e+00,  4.2738e-01, -3.4263e-02,\n",
      "         1.6757e-01, -3.0877e-02,  6.7602e-01,  7.9766e-01, -9.0044e-01,\n",
      "         1.4708e-01,  1.8366e+00,  8.0352e-01,  2.7394e-01,  2.3090e-01,\n",
      "        -8.6506e-01, -3.6357e-01, -2.2211e-01,  7.4435e-01, -4.8689e-01,\n",
      "         2.5621e-01, -1.5109e+00,  2.0811e+00, -2.6663e-01,  4.5646e-01,\n",
      "        -4.4062e-01, -1.7446e-01,  1.6396e+00,  3.6986e-01, -5.3908e-01,\n",
      "        -1.8466e-01, -8.8921e-01, -1.0976e+00,  8.8283e-01,  7.4406e-01,\n",
      "        -2.4945e-01,  2.5362e+00, -1.4620e+00,  2.1674e+00,  1.1511e-01,\n",
      "        -1.8325e-01,  4.2442e-01, -1.2023e-01, -1.1702e+00, -4.3101e-02,\n",
      "         1.2681e-01, -1.0813e+00,  2.4871e+00,  2.9440e-01,  3.9919e-01,\n",
      "        -2.8415e-01,  8.7073e-01,  1.5537e+00, -1.3806e-01,  1.1208e+00,\n",
      "         2.1753e+00,  1.0261e+00, -7.0560e-02,  1.2369e+00, -2.8864e-01,\n",
      "         3.5906e-01, -4.8617e-01, -6.7251e-01,  7.0562e-01, -7.2866e-01,\n",
      "         1.8239e+00,  2.5639e-01,  3.1876e-01, -8.8112e-01, -5.9453e-01,\n",
      "         1.7659e+00,  3.8116e-01, -4.2477e-01,  1.4635e+00,  8.2531e-01,\n",
      "        -7.7232e-02,  5.0972e-01,  7.6466e-01,  1.1482e+00,  4.7100e-01,\n",
      "         2.7478e-01,  2.6568e+00,  1.6093e-01,  8.1569e-01, -1.1459e+00,\n",
      "        -1.5671e+00,  1.2660e+00, -8.2909e-01,  5.5378e-01,  2.9655e-01,\n",
      "         6.3990e-01,  8.4689e-01, -7.6991e-01, -5.5168e-01,  1.8279e-01,\n",
      "         1.4295e-01, -7.9998e-01,  8.0112e-01, -7.3067e-01,  1.2755e-01,\n",
      "         4.5089e-01,  3.1558e-01,  1.1079e+00, -2.3889e-01,  7.0280e-01,\n",
      "         1.1891e+00,  1.8824e+00, -1.3028e+00,  1.0746e-01, -2.1063e+00,\n",
      "         7.9045e-01, -1.0622e+00,  2.3672e-01,  7.2063e-02,  4.1157e-01,\n",
      "        -7.9173e-02,  6.5617e-01,  6.7676e-01,  4.9162e-01,  2.6651e-01,\n",
      "        -4.4940e-01,  1.3236e+00, -6.1010e-01, -1.1714e-01, -1.0956e+00,\n",
      "        -2.5211e-01,  2.6412e-01, -5.6244e-01, -2.0128e-01, -8.4071e-01,\n",
      "         3.2648e-01, -2.9524e-02,  4.7261e-01, -7.0385e-01, -4.5095e-01,\n",
      "        -1.6404e+00,  6.9296e-01,  1.5616e+00, -2.1614e+00,  2.1413e+00,\n",
      "         1.3553e+00, -5.9899e-01, -5.4535e-01,  1.1470e+00, -5.9674e-01,\n",
      "         1.1943e-01,  8.1192e-01,  4.9309e-01,  9.1780e-01,  9.7311e-01,\n",
      "        -2.3779e-01, -2.1455e-01, -2.0275e-01, -1.9906e-01, -2.5821e-01,\n",
      "        -1.2128e-01,  2.8065e-02,  8.3774e-01, -1.6566e+00, -1.2044e+00,\n",
      "        -4.6246e-01,  3.2241e-01, -4.7732e-01,  8.5060e-01,  1.2728e+00,\n",
      "         6.1689e-01, -1.6019e-01, -1.4170e+00, -7.3691e-01,  2.8800e-02,\n",
      "        -2.0656e+00,  4.5778e-01, -1.0017e+00, -4.4263e-01,  7.5992e-01,\n",
      "        -4.0019e-01, -7.0172e-01, -1.9934e-01,  9.4654e-01,  5.1082e-01,\n",
      "        -8.8172e-01,  1.9634e+00, -1.7040e-02, -9.7940e-01, -2.5060e-01,\n",
      "         2.3286e+00, -6.0682e-01, -9.1782e-02,  9.7688e-01,  2.1971e+00,\n",
      "         2.6314e-01,  6.4284e-01,  9.5231e-01,  7.3963e-01,  2.4993e-01,\n",
      "        -3.5515e-01,  4.0766e-01, -6.9184e-01,  2.0599e-01, -5.0592e-02,\n",
      "        -2.8633e-01,  6.3983e-01,  1.2470e-01,  3.3598e-01, -7.4460e-01,\n",
      "        -4.1632e-01,  1.1031e-01,  6.8842e-01,  9.1074e-01,  4.0774e-01,\n",
      "         5.1861e-01,  1.4586e-01,  6.5514e-01,  1.0497e+00,  1.0873e-01,\n",
      "        -2.5422e-01, -3.5213e+00,  2.5822e-01,  2.8830e-01,  1.5232e+00,\n",
      "        -3.5837e-01, -9.2339e-01,  1.0798e+00,  8.2003e-01,  4.5518e-01,\n",
      "         1.3512e+00, -4.5006e-01,  1.1941e+00, -9.2219e-01, -3.5163e-01,\n",
      "        -5.6588e-01, -6.3999e-04,  4.7565e-01, -1.1391e+00,  1.6159e+00,\n",
      "        -1.5083e+00, -2.0313e-01, -2.2908e-02, -9.2437e-01, -7.7440e-01,\n",
      "        -1.2014e-01, -2.6423e-01,  2.3239e-01,  8.5398e-01,  9.6474e-01,\n",
      "        -4.7080e-01, -2.6546e-01,  4.4321e-01,  7.5205e-01, -9.5050e-01,\n",
      "        -1.4183e+00,  1.3610e+00,  4.9172e-01, -8.8746e-01,  5.6487e-01,\n",
      "         1.6774e+00,  2.0109e-01, -7.4961e-01, -5.2640e-01,  1.8136e+00,\n",
      "         1.4779e+00,  1.0931e+00, -7.0960e-01,  8.8124e-01,  1.2791e+00,\n",
      "        -9.0092e-01,  3.5434e-01,  1.4757e+00,  1.0617e+00, -1.1026e+00,\n",
      "         3.1254e-02, -7.1938e-01,  1.9181e-01, -5.9484e-01, -5.0550e-02,\n",
      "        -4.0379e-01, -1.2001e+00, -5.5530e-01,  9.3769e-02,  4.0426e-01,\n",
      "         5.7039e-01, -1.7078e+00, -5.5696e+00,  9.6597e-01, -1.5975e+00,\n",
      "        -5.0947e-01,  1.2915e+00, -1.2400e+00,  8.4634e-01, -2.4607e-01,\n",
      "        -5.8577e-01,  1.2375e+00, -1.0058e+00,  1.8627e-01,  2.0600e-01,\n",
      "         8.4824e-01, -1.0523e+00,  7.4133e-01, -1.1318e+00,  9.1530e-01,\n",
      "         8.2814e-01,  1.1389e+00,  7.1528e-01,  2.1518e+00,  3.6182e-01,\n",
      "        -1.2914e-02,  3.7558e-01, -1.7550e-01, -2.9095e-01,  4.2451e-01,\n",
      "        -4.4926e-01,  1.1149e+00,  4.5106e-01,  1.2188e+00, -1.1361e+00,\n",
      "         1.5387e-01, -8.3496e-02, -1.4450e+00,  5.9026e-01,  1.5560e+00,\n",
      "        -2.4388e-01, -2.6984e-01, -1.4401e-01,  1.8707e+00,  1.5285e+00,\n",
      "        -1.1984e+00,  1.0201e+00, -5.7524e-01, -1.1946e-01,  1.4504e+00,\n",
      "         1.2215e+00,  1.5139e-01, -2.0167e+00,  4.0510e-01,  1.4929e+00,\n",
      "        -5.4596e-01, -8.4858e-01,  1.2047e+00, -7.4683e-02, -1.7684e+00,\n",
      "         4.7179e-01,  8.2556e-01,  1.7216e+00, -6.1923e-01, -3.9332e-01,\n",
      "         6.6955e-01, -1.0773e-01,  2.2362e-01, -2.1947e-03,  4.2518e-01,\n",
      "        -8.1911e-01,  7.0022e-01,  1.4824e+00, -1.1430e+00,  9.8547e-01,\n",
      "        -2.3934e-01,  8.4676e-01,  1.0346e+00,  1.3161e+00,  2.3286e+00,\n",
      "        -9.3676e-02, -4.4837e-01, -1.4270e+00,  8.3490e-01, -9.1071e-01,\n",
      "         1.7665e+00,  1.1520e+00, -1.3079e+00,  1.5837e-01,  6.5319e-03,\n",
      "         1.3581e+00,  1.6953e+00, -1.1441e+00,  1.0625e+00,  7.9216e-01,\n",
      "        -5.6555e-02,  2.2631e+00, -3.9288e-01, -9.4371e-01,  3.6694e-01,\n",
      "        -4.3764e-01,  8.9768e-01,  8.0142e-01,  5.7932e-01,  1.0645e+00,\n",
      "        -1.1275e+00,  1.2109e+00,  3.3829e-01, -3.0146e+00, -3.5758e-02,\n",
      "         2.8762e-01, -1.3387e+00,  8.6288e-01, -1.8525e+00, -8.8965e-01,\n",
      "         7.8217e-01,  1.0310e+00, -7.6667e-01,  6.5482e-01, -3.0523e-01,\n",
      "        -3.6386e-01, -4.1549e-01,  5.1824e-01,  4.5465e-01, -1.2476e+00,\n",
      "         1.7196e+00,  1.2815e+00,  1.5388e-01,  1.6567e+00,  5.6966e-01,\n",
      "         5.8139e-01,  2.1303e+00, -7.1467e-01,  5.1486e-01, -9.1936e-01,\n",
      "        -2.7004e-01, -2.0197e-01, -1.9870e-01, -1.2895e+00, -2.0033e+00,\n",
      "         8.5783e-01, -1.4502e-01, -8.9922e-02, -7.2666e-01, -3.4466e-01,\n",
      "        -3.9614e-01,  1.5499e+00,  3.4891e-01,  1.0848e+00, -2.9357e-02,\n",
      "        -5.0754e-01,  1.3576e+00,  1.2169e+00, -1.4366e-01, -5.0489e-01,\n",
      "        -2.1302e-01,  4.5974e-01, -2.7319e+00, -2.7049e-01, -3.7016e-01,\n",
      "        -2.8187e-01, -2.4903e-01, -1.4284e+00, -6.4017e-01, -8.0612e-01,\n",
      "        -9.1343e-01, -2.2223e-01,  3.3007e-01, -3.7590e-01, -9.1547e-01,\n",
      "        -8.6983e-02,  1.4000e+00,  1.3854e+00,  5.3940e-01,  5.8141e-01,\n",
      "         5.0860e-02, -3.4714e-01, -4.7421e-01, -1.6035e+00,  4.1397e-01,\n",
      "        -1.2125e+00,  4.0013e-01,  2.4710e-01, -3.4936e-01, -2.0202e-01,\n",
      "         6.3854e-01, -1.1483e+00, -9.3708e-01,  5.8992e-01,  1.0569e+00,\n",
      "         3.2500e-01,  5.0334e-01,  1.0579e+00,  1.3065e+00,  1.0260e+00,\n",
      "        -5.2911e-01, -3.6356e+00, -9.2289e-01,  9.6436e-01,  9.0955e-01,\n",
      "         8.8819e-01, -7.9956e-02,  3.0763e-01, -8.2040e-01,  5.9451e-01,\n",
      "         1.0903e+00, -1.0123e+00, -1.6947e+00,  1.4287e+00,  4.5450e-01,\n",
      "        -1.2434e+00, -1.2724e+00, -3.0088e+00,  6.7408e-01,  1.5204e-01,\n",
      "         5.9779e-02,  1.0458e+00,  2.1062e-01,  2.1883e-01, -2.5954e+00,\n",
      "        -3.0023e-01, -5.5525e-01,  1.8862e-02,  1.3797e+00, -6.7477e-01,\n",
      "         1.8881e+00,  1.0713e+00,  1.0710e+00, -1.2661e+00,  8.6746e-02,\n",
      "        -4.7249e-01,  1.0027e+00, -3.1733e-01,  7.6788e-01,  2.8536e-01,\n",
      "        -1.8975e+00,  3.0508e-01,  1.9590e-01, -3.5925e-01, -4.5329e-01,\n",
      "         7.5210e-01,  6.2905e+00,  2.1742e+00,  5.1069e-02, -1.0146e+00,\n",
      "         3.6670e-01,  1.1211e+00,  1.3807e-02, -1.5196e-01,  6.6334e-01,\n",
      "         7.7211e-01, -1.8399e+00,  2.1950e+00, -9.9795e-01, -1.6650e-01,\n",
      "        -9.3371e-01, -1.1487e+00, -6.0442e-01, -3.8167e-01, -6.3974e-01,\n",
      "         1.8929e+00, -3.4243e-01,  3.6730e-01,  4.6468e-01,  2.4817e-01,\n",
      "        -7.3952e-01,  8.8391e-01,  3.9626e-01, -1.5908e+00, -3.5482e-02,\n",
      "        -1.0931e-01,  5.7332e-02,  9.3566e-01], device='cuda:0')]\n",
      "feature_dict:  {'input_ids': tensor([[    0,   118,   206,   109,  1899, 16973,    16,     2]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'decoder_input_ids': tensor([[2]], device='cuda:0')}\n",
      "Device: cuda\n",
      "Model architecture: NISQA_DIM\n",
      "Loaded pretrained model from /work/b0990106x/TextRL/NISQA/weights/nisqa.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load file /work/b0990106x/TextRL/output/0.wav",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/librosa/core/audio.py:149\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n\u001b[1;32m    150\u001b[0m         sr_native \u001b[38;5;241m=\u001b[39m sf_desc\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[0;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[0;31mLibsndfileError\u001b[0m: Error opening '/work/b0990106x/TextRL/output/0.wav': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/work/b0990106x/TextRL/NISQA/nisqa/NISQA_lib.py:2299\u001b[0m, in \u001b[0;36mget_librosa_melspec\u001b[0;34m(file_path, sr, n_fft, hop_length, win_length, n_mels, fmax, ms_channel)\u001b[0m\n\u001b[1;32m   2298\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2299\u001b[0m         y, sr \u001b[38;5;241m=\u001b[39m \u001b[43mlb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2300\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/librosa/core/audio.py:166\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    165\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 166\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/librosa/core/audio.py:190\u001b[0m, in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    189\u001b[0m y \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[1;32m    191\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/audioread/__init__.py:127\u001b[0m, in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBackendClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n",
      "File \u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/audioread/rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/work/b0990106x/TextRL/output/0.wav'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservaton_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/b0990106x/TextRL/textrl/actor.py:126\u001b[0m, in \u001b[0;36mTextRLActor.predict\u001b[0;34m(self, input_item)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mact(obs)\n\u001b[0;32m--> 126\u001b[0m     obs, reward, done, pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     reset \u001b[38;5;241m=\u001b[39m t \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39menv_max_length\n",
      "File \u001b[0;32m/work/b0990106x/TextRL/textrl/environment.py:40\u001b[0m, in \u001b[0;36mTextRLEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action): \u001b[38;5;66;03m# This is the step function that is called by the environment\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     predicted, finish, predicted_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(vocab_id\u001b[38;5;241m=\u001b[39maction)\n\u001b[0;32m---> 40\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_item\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinish\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicted \u001b[38;5;241m=\u001b[39m predicted\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_obs(predicted), reward, finish, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_str\u001b[39m\u001b[38;5;124m\"\u001b[39m: predicted_str}\n",
      "Cell \u001b[0;32mIn[7], line 30\u001b[0m, in \u001b[0;36mMyRLEnv.get_reward\u001b[0;34m(self, input_item, predicted_list, finish)\u001b[0m\n\u001b[1;32m     27\u001b[0m args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr_num_workers\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_workers\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     29\u001b[0m nisqa \u001b[38;5;241m=\u001b[39m nisqaModel(args)\n\u001b[0;32m---> 30\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mnisqa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(prediction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmos_pred\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m, count, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m, reward)\n",
      "File \u001b[0;32m/work/b0990106x/TextRL/NISQA/nisqa/NISQA_model.py:60\u001b[0m, in \u001b[0;36mnisqaModel.predict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDataParallel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)           \n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     y_val_hat, y_val \u001b[38;5;241m=\u001b[39m \u001b[43mNL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_dim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtr_bs_val\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtr_num_workers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     y_val_hat, y_val \u001b[38;5;241m=\u001b[39m NL\u001b[38;5;241m.\u001b[39mpredict_mos(\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds_val, \n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr_bs_val\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdev,\n\u001b[1;32m     72\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr_num_workers\u001b[39m\u001b[38;5;124m'\u001b[39m])                 \n",
      "File \u001b[0;32m/work/b0990106x/TextRL/NISQA/nisqa/NISQA_lib.py:1455\u001b[0m, in \u001b[0;36mpredict_dim\u001b[0;34m(model, ds, bs, dev, num_workers)\u001b[0m\n\u001b[1;32m   1453\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m-> 1455\u001b[0m     y_hat_list \u001b[38;5;241m=\u001b[39m [ [model(xb\u001b[38;5;241m.\u001b[39mto(dev), n_wins\u001b[38;5;241m.\u001b[39mto(dev))\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), yb\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()] \u001b[38;5;28;01mfor\u001b[39;00m xb, yb, (idx, n_wins) \u001b[38;5;129;01min\u001b[39;00m dl]\n\u001b[1;32m   1456\u001b[0m yy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate( y_hat_list, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m )\n\u001b[1;32m   1458\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m yy[\u001b[38;5;241m0\u001b[39m,:,:]\n",
      "File \u001b[0;32m/work/b0990106x/TextRL/NISQA/nisqa/NISQA_lib.py:1455\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1453\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m-> 1455\u001b[0m     y_hat_list \u001b[38;5;241m=\u001b[39m [ [model(xb\u001b[38;5;241m.\u001b[39mto(dev), n_wins\u001b[38;5;241m.\u001b[39mto(dev))\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), yb\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()] \u001b[38;5;28;01mfor\u001b[39;00m xb, yb, (idx, n_wins) \u001b[38;5;129;01min\u001b[39;00m dl]\n\u001b[1;32m   1456\u001b[0m yy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate( y_hat_list, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m )\n\u001b[1;32m   1458\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m yy[\u001b[38;5;241m0\u001b[39m,:,:]\n",
      "File \u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/work/b0990106x/TextRL/NISQA/nisqa/NISQA_lib.py:2168\u001b[0m, in \u001b[0;36mSpeechQualityDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m   2166\u001b[0m     spec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem_list[index]\n\u001b[1;32m   2167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2168\u001b[0m     spec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdouble_ended:               \n\u001b[1;32m   2171\u001b[0m     spec, spec_ref \u001b[38;5;241m=\u001b[39m spec\n",
      "File \u001b[0;32m/work/b0990106x/TextRL/NISQA/nisqa/NISQA_lib.py:2137\u001b[0m, in \u001b[0;36mSpeechQualityDataset._load_spec\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m   2134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdouble_ended:\n\u001b[1;32m   2135\u001b[0m     file_path_ref \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename_column_ref]\u001b[38;5;241m.\u001b[39miloc[index])\n\u001b[0;32m-> 2137\u001b[0m spec \u001b[38;5;241m=\u001b[39m \u001b[43mget_librosa_melspec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43msr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mms_sr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mms_n_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mms_hop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mms_win_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_mels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mms_n_mels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mms_fmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mms_channel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mms_channel\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m   2148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdouble_ended:\n\u001b[1;32m   2149\u001b[0m     spec_ref \u001b[38;5;241m=\u001b[39m get_librosa_melspec(\n\u001b[1;32m   2150\u001b[0m         file_path_ref,\n\u001b[1;32m   2151\u001b[0m         sr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mms_sr,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         fmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mms_fmax\n\u001b[1;32m   2157\u001b[0m         )     \n",
      "File \u001b[0;32m/work/b0990106x/TextRL/NISQA/nisqa/NISQA_lib.py:2301\u001b[0m, in \u001b[0;36mget_librosa_melspec\u001b[0;34m(file_path, sr, n_fft, hop_length, win_length, n_mels, fmax, ms_channel)\u001b[0m\n\u001b[1;32m   2299\u001b[0m         y, sr \u001b[38;5;241m=\u001b[39m lb\u001b[38;5;241m.\u001b[39mload(file_path, sr\u001b[38;5;241m=\u001b[39msr)\n\u001b[1;32m   2300\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m-> 2301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not load file \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(file_path))\n\u001b[1;32m   2303\u001b[0m hop_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(sr \u001b[38;5;241m*\u001b[39m hop_length)\n\u001b[1;32m   2304\u001b[0m win_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(sr \u001b[38;5;241m*\u001b[39m win_length)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load file /work/b0990106x/TextRL/output/0.wav"
     ]
    }
   ],
   "source": [
    "actor.predict(observaton_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1. Agent to Environment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path\n",
    "agent_input_dir = f'{base_path}/data-encodec'\n",
    "agent_output_dir = f'{base_path}/output'\n",
    "env_input_dir = agent_output_dir\n",
    "env_output_dir = agent_input_dir\n",
    "\n",
    "ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "nar_checkpoint = \"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:  Play the audio twice.\n",
      "Transcription:  There is even a white row of beehives in the orchard, under the walnut trees.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "ar_model = BartForConditionalGeneration.from_pretrained(ar_checkpoint)\n",
    "ar_model.to(device)\n",
    "\n",
    "dataset = load_from_disk(agent_input_dir)\n",
    "instruction_ids = ar_tokenizer(dataset[\"instruction\"][0])[\"input_ids\"][1 : -1]\n",
    "transcription_ids = ar_tokenizer(dataset[\"transcription\"][0])[\"input_ids\"][1 : -1]\n",
    "instruction = dataset[\"instruction\"][0]\n",
    "transcription = dataset[\"transcription\"][0]\n",
    "\n",
    "print(\"Instruction: \", instruction)\n",
    "print(\"Transcription: \", transcription)\n",
    "\n",
    "# for i in range(len(instruction_ids)):\n",
    "#     print(\"Instruction(cascade): \", ar_tokenizer.decode(instruction_ids[i]))\n",
    "# for i in range(len(transcription_ids)):\n",
    "#     print(\"Transcription(cascade): \", ar_tokenizer.decode(transcription_ids[i]))\n",
    "    \n",
    "observation_list = [{'input': 0, 'transcription': transcription, 'instruction': instruction, 'dataset': dataset}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = VcRLEnv(model, tokenizer, observation_list, 100, 2, 1, env_input_dir, env_output_dir, instruction, transcription)\n",
    "# actor = VcActor(env, ar_model, ar_tokenizer, ar_checkpoint, nar_checkpoint, agent_input_dir, agent_output_dir, device, observation_list, 100, 2)\n",
    "# actor.predict(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# define env, actor, and agent\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# actor = TextRLActor(env,model,tokenizer)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mVcRLEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_input_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_output_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranscription\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m actor \u001b[38;5;241m=\u001b[39m VcActor(env, ar_model, ar_tokenizer, ar_checkpoint, nar_checkpoint, agent_input_dir, agent_output_dir, device, observation_list, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m, in \u001b[0;36mVcRLEnv.__init__\u001b[0;34m(self, model, tokenizer, observation_input, max_length, compare_sample, unfreeze_layer_from_past, env_input_dir, env_output_dir, instruction, transcription)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction \u001b[38;5;241m=\u001b[39m instruction\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranscription \u001b[38;5;241m=\u001b[39m transcription\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen_stop_toks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     24\u001b[0m logging\u001b[38;5;241m.\u001b[39mdisable(sys\u001b[38;5;241m.\u001b[39mmaxsize)\n",
      "File \u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 79\u001b[0m, in \u001b[0;36mVcRLEnv.reset\u001b[0;34m(self, input_item)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_item \u001b[38;5;241m=\u001b[39m input_item\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredicted\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 102\u001b[0m, in \u001b[0;36mVcRLEnv._get_obs\u001b[0;34m(self, predicted)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m([k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnamed_parameters() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m k]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 102\u001b[0m         feature_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgat_obs_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_item\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(p_text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    107\u001b[0m             decoder_input_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id] \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m    108\u001b[0m                                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(p_text)\n",
      "File \u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2538\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2537\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2538\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2596\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2593\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2596\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2597\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2598\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2599\u001b[0m     )\n\u001b[1;32m   2601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2602\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2605\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "# define env, actor, and agent\n",
    "# actor = TextRLActor(env,model,tokenizer)\n",
    "env = VcRLEnv(model, tokenizer, observation_list, 100, 2, 1, env_input_dir, env_output_dir, instruction, transcription)\n",
    "actor = VcActor(env, ar_model, ar_tokenizer, ar_checkpoint, nar_checkpoint, agent_input_dir, agent_output_dir, device, observation_list, 100, 2)\n",
    "\n",
    "# for i in range(10):\n",
    "#     print(\"Step: \", i)\n",
    "#     actor.predict(i)\n",
    "#     env.step(i)\n",
    "#     actor.agent_ppo(update_interval=100, minibatch_size=3, epochs=10)\n",
    "    \n",
    "# agent = vc_inference.run(args, agent_input_dir, agent_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = actor.agent_ppo(update_interval=100, minibatch_size=3, epochs=10)\n",
    "pfrl.experiments.train_agent_with_evaluation(\n",
    "    agent,\n",
    "    env,\n",
    "    steps=300,\n",
    "    eval_n_steps=None,\n",
    "    eval_n_episodes=1,       \n",
    "    train_max_episode_len=100,  \n",
    "    eval_interval=10,\n",
    "    outdir='output', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor.predict(observaton_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2. Environment to Agent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward = env.get_reward()\n",
    "# print(\"reward\", reward)\n",
    "env._predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of agent (wav) + instruction + transcription"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
