{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "At5gZSqIG1ah"
   },
   "source": [
    "# Controllable generation via RL about text-guided voice conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# load the model\n",
    "ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "nar_checkpoint = \"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "ar_model = BartForConditionalGeneration.from_pretrained(ar_checkpoint)\n",
    "nar_tokenizer = AutoTokenizer.from_pretrained(nar_checkpoint)\n",
    "nar_model = NARBartForConditionalGeneration.from_pretrained(nar_checkpoint)\n",
    "ar_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "now = datetime.now()\n",
    "ts = now.strftime(\"%m%d-%H%M\")\n",
    "print(\"timestamp:\", ts)\n",
    "\n",
    "# define the path\n",
    "base_path = \"/work/b0990106x/TextRL\"\n",
    "agent_input_dir = f\"{base_path}/data-encodec\"\n",
    "agent_output_dir = f\"{base_path}/output/{ts}\"\n",
    "env_input_dir = agent_output_dir\n",
    "env_output_dir = agent_input_dir\n",
    "\n",
    "if not os.path.exists(agent_output_dir):\n",
    "    os.makedirs(agent_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataset = load_from_disk(agent_input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_src_encodec_layers = []\n",
    "all_src_encodec = []\n",
    "all_instruction = []\n",
    "# all_instruction_ids = []\n",
    "\n",
    "layer_len = 8\n",
    "data_len = 3\n",
    "# data_len = len(dataset)\n",
    "print(\"data_len:\", data_len)\n",
    "\n",
    "for i in range(layer_len):\n",
    "    all_src_encodec_layers.append(dataset[f\"src_encodec_{i}\"])\n",
    "\n",
    "for i in range(data_len):\n",
    "    src_encodec = []\n",
    "    for j in range(layer_len):\n",
    "        src_encodec.append(all_src_encodec_layers[j][i])\n",
    "    all_src_encodec.append(src_encodec)\n",
    "\n",
    "    all_instruction.append(dataset[\"instruction\"][i])\n",
    "    # all_instruction_ids.append(ar_tokenizer(all_instruction[i])[\"input_ids\"][1 : -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the length of all src encodec\n",
    "for i in range(data_len):\n",
    "    print(f\"src_encodec_{i} len:\", len(all_src_encodec[i][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine the path (one can run the code from here when the model is already loaded)\n",
    "now = datetime.now()\n",
    "ts = now.strftime(\"%m%d-%H%M\")\n",
    "print(\"timestamp:\", ts)\n",
    "\n",
    "agent_output_dir = f\"{base_path}/output/{ts}\"\n",
    "env_input_dir = agent_output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import textrl\n",
    "reload(textrl)\n",
    "\n",
    "from textrl import TextRLEnv, TextRLActor\n",
    "from NISQA.nisqa.NISQA_model import nisqaModel\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/work/b0990106x/TextRL/vc\") \n",
    "from vc.trainer_encodec_vc_inference import get_ar_prediction_v2\n",
    "\n",
    "class MyRLEnv(TextRLEnv):\n",
    "    def get_reward(self, _, predicted_list, finish):\n",
    "        reward = 0\n",
    "        if finish or len(predicted_list[0]) >= self.env_max_length:\n",
    "        # if finish or len(predicted_list) >= self.env_max_length:\n",
    "            print(\"Length of predicted_list:\", len(predicted_list))\n",
    "            print(\"predicted_list:\", predicted_list)\n",
    "            reward = len(predicted_list[0])\n",
    "            # try:\n",
    "            #     predicted_tokens = predicted_list[0][1:-1]\n",
    "            #     predicted_ids = self.tokenizer.convert_tokens_to_ids([f\"{u}\" for u in predicted_tokens])\n",
    "            #     # print(\"predicted_ids:\", predicted_ids)\n",
    "\n",
    "            #     decode_ar = get_ar_prediction_v2(\n",
    "            #         self.args_predict,\n",
    "            #         predicted_ids,\n",
    "            #         self.nar_model,\n",
    "            #         self.tokenizer,\n",
    "            #         self.nar_tokenizer,\n",
    "            #         self.single_src_encodec,\n",
    "            #         self.single_instruction,\n",
    "            #         self.episode_counter,\n",
    "            #     )\n",
    "            #     # print(\"decode_ar:\", decode_ar)\n",
    "                \n",
    "            #     # use nisqa to get the reward\n",
    "            #     args_nisqa = {\n",
    "            #         \"mode\": \"predict_file\",\n",
    "            #         \"pretrained_model\": f\"{base_path}/NISQA/weights/nisqa.tar\",\n",
    "            #         \"deg\": f\"{base_path}/output/{ts}/example.wav\",\n",
    "            #         \"data_dir\": None,\n",
    "            #         \"output_dir\": f\"{base_path}/NISQA/result/\",\n",
    "            #         \"csv_file\": None,\n",
    "            #         \"csv_deg\": None,\n",
    "            #         \"num_workers\": 0,\n",
    "            #         \"bs\": 1,\n",
    "            #         \"ms_channel\": None,\n",
    "            #     }\n",
    "            #     args_nisqa[\"tr_bs_val\"] = args_nisqa[\"bs\"]\n",
    "            #     args_nisqa[\"tr_num_workers\"] = args_nisqa[\"num_workers\"]\n",
    "\n",
    "            #     nisqa = nisqaModel(args_nisqa)\n",
    "            #     prediction = nisqa.predict()\n",
    "            #     reward = float(prediction[\"mos_pred\"].iloc[0])*10\n",
    "            #     # reward = float(prediction[\"mos_pred\"].iloc[0])-3.0\n",
    "            #     print(\n",
    "            #         \"Length of predicted_list:\",\n",
    "            #         len(predicted_list[0]),\n",
    "            #         \", Reward:\",\n",
    "            #         reward,\n",
    "            #     )\n",
    "\n",
    "            # except Exception as e:\n",
    "            #     print(\"Error:\", e)\n",
    "            #     reward = 0\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_list = []\n",
    "for i in range(data_len):\n",
    "    observation_list.append(\n",
    "        {\n",
    "            \"input\": \"\",\n",
    "            \"src_encodec\": all_src_encodec[i],\n",
    "            \"instruction\": all_instruction[i],\n",
    "        }\n",
    "    )\n",
    "    # print(\"src_encodec:\", observation_list[i][\"src_encodec\"][0])\n",
    "    # print(\"instruction:\", all_instruction[i])\n",
    "    \n",
    "# pop the first one\n",
    "observation_list.pop(0)\n",
    "all_instruction.pop(0)\n",
    "observation_list.pop(0)\n",
    "all_instruction.pop(0)\n",
    "print(\"observation_list:\", observation_list)\n",
    "print(\"all_instruction:\", all_instruction)\n",
    "\n",
    "# for i in range(data_len):\n",
    "#     observation_list.append({'input': \"\", 'src_encodec': all_src_encodec[i], 'instruction': all_instruction[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"observation_list:\", observation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args_predict = SimpleNamespace(\n",
    "    output_path=f\"{base_path}/output/{ts}/example.wav\", seed=0, device=\"cuda\"\n",
    ")\n",
    "\n",
    "env = MyRLEnv(\n",
    "    ar_model,\n",
    "    ar_tokenizer,\n",
    "    nar_model,\n",
    "    nar_tokenizer,\n",
    "    args_predict,\n",
    "    observation_input=observation_list,\n",
    "    compare_sample=1,\n",
    ")\n",
    "actor = TextRLActor(env = env, model = ar_model, tokenizer = ar_tokenizer)\n",
    "# agent = actor.agent_ppo(update_interval=1800, minibatch_size=256, epochs=10, lr=3e-8)\n",
    "# agent = actor.agent_ppo(update_interval=1200, minibatch_size=128, epochs=10)\n",
    "# agent = actor.agent_ppo(update_interval=1000, minibatch_size=128, epochs=10, lr=3e-8)\n",
    "update_interval = 1000\n",
    "minibatch_size = 512\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "# agent = actor.agent_ppo(update_interval=1000, minibatch_size=512, epochs=10, lr=0.001)\n",
    "# agent = actor.agent_ppo(update_interval=1000, minibatch_size=512, epochs=10, lr=0.01, entropy_coef=0.1)\n",
    "# agent = actor.agent_ppo(update_interval=1000, minibatch_size=2048, epochs=1, lr=0.01, entropy_coef=0.1)\n",
    "# agent = actor.agent_ppo(update_interval=100, minibatch_size=1024, epochs=1, lr=0.05, entropy_coef=0.5)\n",
    "# agent = actor.agent_ppo(update_interval=100, minibatch_size=1024, epochs=1, lr=0.01, entropy_coef=0.5)\n",
    "# agent = actor.agent_ppo(update_interval=2048, minibatch_size=32, epochs=1, lr=0.01, entropy_coef=0.1)\n",
    "# agent = actor.agent_ppo(update_interval=2048, minibatch_size=32, epochs=1, lr=0.01)\n",
    "agent = actor.agent_ppo(update_interval=1, minibatch_size=1, epochs=1, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "output_log_path = f\"log/log_{ts}.log\"\n",
    "output_file_path = f\"log/output_{ts}.txt\"\n",
    "\n",
    "if not os.path.exists(\"log\"):\n",
    "    os.makedirs(\"log\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "handlers = logger.handlers[:]\n",
    "for handler in handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "file_handler = logging.FileHandler(output_log_path)\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FBysk9MiHR2D",
    "outputId": "4086dcd7-6d19-44bc-e1b0-fa764f873301",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import pfrl\n",
    "\n",
    "start_time = time.time()\n",
    "pfrl_outdir = f\"ckpt/train_{ts}\"\n",
    "\n",
    "with open(output_file_path, \"w\") as f:\n",
    "    original_stdout = sys.stdout\n",
    "    sys.stdout = f\n",
    "    print(f\"update_interval = {update_interval}, minibatch_size = {minibatch_size}, epochs = {epochs}, lr = {lr}\")\n",
    "    pfrl.experiments.train_agent_with_evaluation(\n",
    "        agent,\n",
    "        env,\n",
    "        steps=1000000,\n",
    "        eval_n_steps=None,\n",
    "        eval_n_episodes=1,\n",
    "        train_max_episode_len=10000,\n",
    "        eval_interval=1000,\n",
    "        outdir=pfrl_outdir,\n",
    "        logger=logger,\n",
    "        use_tensorboard=True,\n",
    "        checkpoint_freq=5000,\n",
    "    )\n",
    "    sys.stdout = original_stdout   \n",
    "    \n",
    "print(\"Output has been written to\", output_file_path)\n",
    "print(\"used time: \", time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dpAwe42ES-5w",
    "outputId": "bb12c0d2-1916-4076-8f98-b20d2a2e4e57"
   },
   "outputs": [],
   "source": [
    "agent.load(pfrl_outdir + \"/best\")\n",
    "actor.predict(observation_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import read_pickle\n",
    "# ts = \"0430-1250\"\n",
    "# # len of the pickle file\n",
    "# dir_path = f\"{base_path}/replay_buffer/{ts}\"\n",
    "# length = len(os.listdir(dir_path))\n",
    "# for i in range(length):\n",
    "#     pickle_file = f\"replay_buffer_update_{i}.pkl\"\n",
    "#     pickle_data = read_pickle.load_pickle(file_path = f'{base_path}/replay_buffer/{ts}/{pickle_file}')\n",
    "#     length_of_pickle_data = len(pickle_data[0])\n",
    "#     length_of_pickle_data1 = len(pickle_data[1])\n",
    "#     if len(pickle_data)>= 3:\n",
    "#         length_of_pickle_data2 = len(pickle_data[2])\n",
    "#         print(f\"length of {pickle_file}: {length_of_pickle_data} and {length_of_pickle_data1} and {length_of_pickle_data2}\")\n",
    "#     else:\n",
    "#         print(f\"length of {pickle_file}: {length_of_pickle_data} and {length_of_pickle_data1}\") \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# print(tf.__version__)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c6c77b12b02a1c2aaa91a9fb9cc35bb3c4bbfb7b716f83ac7b2b57ffb1247cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
