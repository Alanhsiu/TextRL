{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pfrl@git+https://github.com/voidful/pfrl.git\n",
    "%pip install textrl==0.2.15\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\")\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pfrl\n",
    "from textrl import TextRLEnv,TextRLActor\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer, AutoModelWithLMHead\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "from NISQA.nisqa.NISQA_model import nisqaModel\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/work/b0990106x/TextRL'\n",
    "\n",
    "# print(\"model\", model)\n",
    "# print(\"tokenizer\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RL Environment: NISQA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRLEnv(TextRLEnv):\n",
    "    def __init__(self, model, tokenizer, device=\"cuda\", observation_input=[], max_length=100, compare_sample=2):\n",
    "        super().__init__(model, tokenizer, observation_input, max_length, compare_sample)\n",
    "        self.device = torch.device(device)\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.observation_input = observation_input\n",
    "\n",
    "        \n",
    "    def get_reward(self, input_path=None, output_dir=None): # predicted will be the list of predicted token\n",
    "        args = {\n",
    "            'mode': 'predict_file',  # For example, 'predict_file', 'predict_dir', or 'predict_csv'\n",
    "            'pretrained_model': f'{base_path}/NISQA/weights/nisqa.tar',  # Name of the pretrained model file\n",
    "            'deg': f'{base_path}/NISQA/wav/test.wav',  # Path to the speech file if mode is predict_file\n",
    "            'data_dir': None,  # Folder with speech files if mode is predict_dir\n",
    "            'output_dir': f'{base_path}/NISQA/result',  # Folder to output results.csv\n",
    "            'csv_file': None,  # Name of the csv file if mode is predict_csv\n",
    "            'csv_deg': None,  # Column name in csv with file names/paths if mode is predict_csv\n",
    "            'num_workers': 0,  # Number of workers for PyTorch's DataLoader\n",
    "            'bs': 1,  # Batch size for predicting\n",
    "            'ms_channel': None,  # Audio channel in case of a stereo file, if needed\n",
    "        }\n",
    "\n",
    "        if input_path is not None:\n",
    "            args['deg'] = input_path\n",
    "\n",
    "        args['tr_bs_val'] = args['bs']\n",
    "        args['tr_num_workers'] = args['num_workers']\n",
    "        \n",
    "        nisqa = nisqaModel(args)\n",
    "        prediction = nisqa.predict()\n",
    "        reward = int(prediction['mos_pred'].iloc[0])\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RL Agent: Text-Instruction-Guided Voice Conversion Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from encodec import EncodecModel\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from transformers import (AutoTokenizer, BartForConditionalGeneration,\n",
    "                          BatchEncoding)\n",
    "import vc.trainer_encodec_vc_inference as vc_inference\n",
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    dataset=\"lca0503/soxdata_small_encodec\",\n",
    "    splits=[\"train\"],\n",
    "    ground_truth_only=False,\n",
    "    cascade_ar_nar=True,\n",
    "    nar_model_only=False,\n",
    "    ground_truth_model_name=\"voidful/bart-base-unit\",\n",
    "    ar_checkpoint=\"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\",\n",
    "    nar_checkpoint=\"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\",\n",
    "    ground_truth_output_path=\"output_wav/vc/ground_truth/train_1.wav\",\n",
    "    cascade_output_path=\"output_wav/vc/ar_nar_cascade/train_1.wav\",\n",
    "    nar_output_path=\"output_wav/vc/nar/train_1.wav\",\n",
    "    seed=0,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets\n",
    "# from datasets import load_from_disk ,load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"lca0503/soxdata_encodec\")\n",
    "# dataset.save_to_disk(\"data\")\n",
    "\n",
    "# dataset = load_dataset(\"lca0503/soxdata_encodec\", split=\"+\".join([\"train\"]))\n",
    "# dataset = dataset.filter(lambda x : len(x[f\"src_encodec_0\"]) <= 700)\n",
    "# dataset = dataset.shuffle(0).select(range(1))\n",
    "\n",
    "# dataset.save_to_disk(\"data-encodec\")\n",
    "# dataset = load_from_disk(\"data-encodec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Agent to Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_input_dir = f'{base_path}/data-encodec'\n",
    "agent_output_path = f'{base_path}/output/test.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ar_tokenizer = AutoTokenizer.from_pretrained(args.ar_checkpoint)\n",
    "ar_model = BartForConditionalGeneration.from_pretrained(args.ar_checkpoint)\n",
    "ar_model.to(device)\n",
    "\n",
    "dataset = load_from_disk(agent_input_dir)\n",
    "instruction_ids = ar_tokenizer(dataset[\"instruction\"][0])[\"input_ids\"][1 : -1]\n",
    "transcription_ids = ar_tokenizer(dataset[\"transcription\"][0])[\"input_ids\"][1 : -1]\n",
    "\n",
    "for i in range(len(instruction_ids)):\n",
    "    print(\"Instruction(cascade): \", ar_tokenizer.decode(instruction_ids[i]))\n",
    "for i in range(len(transcription_ids)):\n",
    "    print(\"Transcription(cascade): \", ar_tokenizer.decode(transcription_ids[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = vc_inference.run(args, agent_input_dir, agent_output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Environment to Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_input_path = agent_output_path\n",
    "env_output_dir = agent_input_dir\n",
    "observation_list = [{'input': str(transcription_ids), 'output': str(instruction_ids)}] # need to modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation_list = [{'input': 'Hello, how are you?'}]\n",
    "# input_path = f'{base_path}/NISQA/wav/test.wav'\n",
    "# output_dir = f'{base_path}/NISQA/result'\n",
    "\n",
    "env = MyRLEnv(model, tokenizer ,observation_input=observation_list, max_length=100, compare_sample=2)\n",
    "print(env.get_reward())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
