{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting pfrl@ git+https://github.com/voidful/pfrl.git\n",
      "  Cloning https://github.com/voidful/pfrl.git to /tmp/pip-install-o42eun3k/pfrl_707eb97a1d2f43b7a133d6947d0c8699\n",
      "  Running command git clone --filter=blob:none -q https://github.com/voidful/pfrl.git /tmp/pip-install-o42eun3k/pfrl_707eb97a1d2f43b7a133d6947d0c8699\n",
      "  Resolved https://github.com/voidful/pfrl.git to commit bd679022d91902a3c88066d2646ad69b30d5684a\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.3.0 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from pfrl@ git+https://github.com/voidful/pfrl.git) (2.2.1)\n",
      "Requirement already satisfied: gym>=0.9.7 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from pfrl@ git+https://github.com/voidful/pfrl.git) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from pfrl@ git+https://github.com/voidful/pfrl.git) (1.20.3)\n",
      "Requirement already satisfied: pillow in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from pfrl@ git+https://github.com/voidful/pfrl.git) (8.4.0)\n",
      "Requirement already satisfied: filelock in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from pfrl@ git+https://github.com/voidful/pfrl.git) (3.13.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from gym>=0.9.7->pfrl@ git+https://github.com/voidful/pfrl.git) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from gym>=0.9.7->pfrl@ git+https://github.com/voidful/pfrl.git) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from gym>=0.9.7->pfrl@ git+https://github.com/voidful/pfrl.git) (7.0.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (10.3.2.106)\n",
      "Requirement already satisfied: jinja2 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (3.1.3)\n",
      "Requirement already satisfied: networkx in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (11.4.5.107)\n",
      "Requirement already satisfied: fsspec in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (12.1.105)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (4.9.0)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (2.2.0)\n",
      "Requirement already satisfied: sympy in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (1.12)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (12.3.101)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gym>=0.9.7->pfrl@ git+https://github.com/voidful/pfrl.git) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from jinja2->torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from sympy->torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: textrl==0.2.15 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (0.2.15)\n",
      "Requirement already satisfied: gym in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from textrl==0.2.15) (0.26.2)\n",
      "Requirement already satisfied: transformers in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from textrl==0.2.15) (4.28.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from gym->textrl==0.2.15) (7.0.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from gym->textrl==0.2.15) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from gym->textrl==0.2.15) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from gym->textrl==0.2.15) (1.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from transformers->textrl==0.2.15) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from transformers->textrl==0.2.15) (0.20.3)\n",
      "Requirement already satisfied: requests in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from transformers->textrl==0.2.15) (2.31.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from transformers->textrl==0.2.15) (23.2)\n",
      "Requirement already satisfied: filelock in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from transformers->textrl==0.2.15) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from transformers->textrl==0.2.15) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from transformers->textrl==0.2.15) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from transformers->textrl==0.2.15) (0.13.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers->textrl==0.2.15) (4.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers->textrl==0.2.15) (2023.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gym->textrl==0.2.15) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from requests->transformers->textrl==0.2.15) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from requests->transformers->textrl==0.2.15) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from requests->transformers->textrl==0.2.15) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from requests->transformers->textrl==0.2.15) (3.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (8.1.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from ipywidgets) (7.22.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: decorator in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: backcall in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (69.1.0)\n",
      "Requirement already satisfied: pickleshare in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: pygments in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pfrl@git+https://github.com/voidful/pfrl.git\n",
    "%pip install textrl==0.2.15\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(59481, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(59481, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(59481, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=59481, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\")\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pfrl\n",
    "from textrl import TextRLEnv,TextRLActor\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer, AutoModelWithLMHead\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "from NISQA.nisqa.NISQA_model import nisqaModel\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/work/b0990106x/TextRL'\n",
    "\n",
    "# print(\"model\", model)\n",
    "# print(\"tokenizer\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RL Environment: NISQA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.2.1 available.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import logging\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "from torch import autocast\n",
    "from vc.wav_to_arrow import process_audio\n",
    "\n",
    "class VcRLEnv(gym.Env):\n",
    "    def __init__(self, model, tokenizer, observation_input=[], max_length=100, compare_sample=2, unfreeze_layer_from_past=0, env_input_dir=None, env_output_dir=None, instruction=\"\", transcription=\"\"):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.observation_space = observation_input\n",
    "        self.compare_sample = compare_sample\n",
    "        self.unfreeze_layer_from_past = 1 if unfreeze_layer_from_past else 0\n",
    "        self.env_max_length = min(max(self.model.config.max_length, self.tokenizer.model_max_length), max_length)\n",
    "        self.env_input_dir = env_input_dir\n",
    "        self.env_output_dir = env_output_dir\n",
    "        self.instruction = instruction\n",
    "        self.transcription = transcription\n",
    "        # self.reset()\n",
    "        \n",
    "        self.gen_stop_toks = []\n",
    "        logging.disable(sys.maxsize)\n",
    "        if self.tokenizer.sep_token:\n",
    "            self.gen_stop_toks.append(self.tokenizer.sep_token)\n",
    "        if self.tokenizer.eos_token:\n",
    "            self.gen_stop_toks.append(self.tokenizer.eos_token)\n",
    "        logging.disable(logging.NOTSET)\n",
    "        \n",
    "    def step(self, count):\n",
    "        reward = self._predict(count)\n",
    "        self._get_obs(count)\n",
    "        return reward\n",
    "    \n",
    "    def get_reward(self, input_path=None, output_dir=None, count=0): # predicted will be the list of predicted token\n",
    "        args = {\n",
    "            'mode': 'predict_file', \n",
    "            'pretrained_model': f'{base_path}/NISQA/weights/nisqa.tar', \n",
    "            'deg': f'{base_path}/NISQA/wav/{count}.wav', \n",
    "            'data_dir': None, \n",
    "            'output_dir': f'{base_path}/NISQA/result',\n",
    "            'csv_file': None, \n",
    "            'csv_deg': None,  \n",
    "            'num_workers': 0, \n",
    "            'bs': 1,\n",
    "            'ms_channel': None\n",
    "        }\n",
    "\n",
    "        if input_path is not None:\n",
    "            args['deg'] = input_path\n",
    "\n",
    "        args['tr_bs_val'] = args['bs']\n",
    "        args['tr_num_workers'] = args['num_workers']\n",
    "        \n",
    "        nisqa = nisqaModel(args)\n",
    "        prediction = nisqa.predict()\n",
    "        reward = float(prediction['mos_pred'].iloc[0])\n",
    "        return reward\n",
    "    \n",
    "    # @autocast('cuda')\n",
    "    # def reset(self, input_item=None):\n",
    "    #     self.predicted = [[]] * self.compare_sample\n",
    "    #     self.predicted_end = [False] * self.compare_sample\n",
    "    #     self.input_item = {\"input\": \"\"}\n",
    "    #     return self._get_obs(self.predicted)\n",
    "    \n",
    "    def _get_obs(self, count):\n",
    "        # get the observation\n",
    "        audio_path = f\"{self.env_input_dir}/{count}.wav\"\n",
    "\n",
    "        process_audio(source_audio_path=audio_path, output_dir = self.env_output_dir, temp_dir=\"/work/b0990106x/TextRL/vc/data/temp\", instruction=self.instruction, transcription=self.transcription)\n",
    "        return count\n",
    "        \n",
    "    def _predict(self, count):\n",
    "        self.get_reward(count)\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import autocast\n",
    "from transformers import (AutoTokenizer, BartForConditionalGeneration,\n",
    "                          BatchEncoding)\n",
    "from vc.trainer_encodec_vc_inference import cascade_ar_nar, convert_to_encode_code,synthesize_audio\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import textrl.actor\n",
    "\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pfrl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pfrl.agents.ppo import _elementwise_clip\n",
    "from pfrl.utils.mode_of_distribution import mode_of_distribution\n",
    "from torch import autocast\n",
    "\n",
    "\n",
    "class VcPPOAgent(pfrl.agents.PPO):\n",
    "    def __init__(self, model, tokenizer, device=\"cuda\", observation_input=[], max_length=100, compare_sample=2):\n",
    "        env = VcRLEnv(model, tokenizer, device, observation_input, max_length, compare_sample)\n",
    "        model = VcActor(model, tokenizer, device, observation_input, max_length, compare_sample)\n",
    "        super().__init__(env=env, model=model)\n",
    "        \n",
    "class VcActor(TextRLActor):\n",
    "    def __init__(self, model, tokenizer,ar_checkpoint, nar_checkpoint, input_dir, output_dir, device, observation_input=[], max_length=100, compare_sample=2):\n",
    "        \n",
    "        \n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.observation_input = observation_input\n",
    "        self.ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "        self.ar_model = BartForConditionalGeneration.from_pretrained(ar_checkpoint)\n",
    "        self.nar_tokenizer = AutoTokenizer.from_pretrained(nar_checkpoint)\n",
    "        self.nar_model = NARBartForConditionalGeneration.from_pretrained(nar_checkpoint)\n",
    "        self.device = torch.device(device)\n",
    "        self.ar_model.to(self.device)\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    @autocast('cuda')\n",
    "    def predict(self, count):\n",
    "        # use the model to predict the next token\n",
    "        layer_list = cascade_ar_nar(self.ar_model, self.nar_model, self.ar_tokenizer, self.nar_tokenizer, self.input_dir, self.device)\n",
    "        encodec_code = convert_to_encode_code(self.nar_tokenizer, layer_list)    \n",
    "        audio = synthesize_audio(encodec_code, self.device)\n",
    "        output_path = f\"{self.output_dir}/{count}.wav\"\n",
    "        sf.write(output_path, np.ravel(audio), samplerate=24000)\n",
    "\n",
    "    def agent_ppo(self, update_interval=10, minibatch_size=3000, epochs=20, lr=3e-6):\n",
    "        policy = torch.nn.Sequential(\n",
    "            self.middle_model,\n",
    "            self.remaining_model,\n",
    "            self.converter,\n",
    "            textrl.actor.SoftmaxCategoricalHead(self.env,\n",
    "                                   temperature=self.temperature,\n",
    "                                   top_k=self.top_k,\n",
    "                                   top_p=self.top_p)\n",
    "        )\n",
    "        vf = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.obs_size, self.obs_size // 2),\n",
    "            torch.nn.Linear(self.obs_size // 2, self.obs_size // 4),\n",
    "            torch.nn.Linear(self.obs_size // 4, 1)\n",
    "        )\n",
    "        model = pfrl.nn.Branched(policy, vf)\n",
    "        if isinstance(self.optimizer, str):\n",
    "            if self.optimizer.lower() == 'adamw':\n",
    "                opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "            else:\n",
    "                opt = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "        else:\n",
    "            opt = self.optimizer\n",
    "        model = model.cuda()\n",
    "        agent = textrl.actor.TextPPO(\n",
    "            model,\n",
    "            opt,\n",
    "            gpu=self.gpu_id,\n",
    "            update_interval=update_interval,\n",
    "            minibatch_size=minibatch_size,\n",
    "            epochs=epochs,\n",
    "            clip_eps_vf=None,\n",
    "            entropy_coef=0,\n",
    "            gamma=0.95,  # https://arxiv.org/abs/2210.01241\n",
    "            lambd=1,\n",
    "            max_grad_norm=1.0,\n",
    "            standardize_advantages=True,\n",
    "            act_deterministically=self.act_deterministically\n",
    "        )\n",
    "        self.agent = agent\n",
    "        return agent\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RL Agent: Text-Instruction-Guided Voice Conversion Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from encodec import EncodecModel\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from transformers import (AutoTokenizer, BartForConditionalGeneration,\n",
    "                          BatchEncoding)\n",
    "import vc.trainer_encodec_vc_inference as vc_inference\n",
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    dataset=\"lca0503/soxdata_small_encodec\",\n",
    "    splits=[\"train\"],\n",
    "    ground_truth_only=False,\n",
    "    cascade_ar_nar=True,\n",
    "    nar_model_only=False,\n",
    "    ground_truth_model_name=\"voidful/bart-base-unit\",\n",
    "    ar_checkpoint=\"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\",\n",
    "    nar_checkpoint=\"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\",\n",
    "    ground_truth_output_path=\"output_wav/vc/ground_truth/train_1.wav\",\n",
    "    cascade_output_path=\"output_wav/vc/ar_nar_cascade/train_1.wav\",\n",
    "    nar_output_path=\"output_wav/vc/nar/train_1.wav\",\n",
    "    seed=0,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "# modify ar_checkpoint in args\n",
    "args.ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets\n",
    "# from datasets import load_from_disk ,load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"lca0503/soxdata_encodec\")\n",
    "# dataset.save_to_disk(\"data\")\n",
    "\n",
    "# dataset = load_dataset(\"lca0503/soxdata_encodec\", split=\"+\".join([\"train\"]))\n",
    "# dataset = dataset.filter(lambda x : len(x[f\"src_encodec_0\"]) <= 700)\n",
    "# dataset = dataset.shuffle(0).select(range(1))\n",
    "\n",
    "# dataset.save_to_disk(\"data-encodec\")\n",
    "# dataset = load_from_disk(\"data-encodec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1. Agent to Environment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path\n",
    "agent_input_dir = f'{base_path}/data-encodec'\n",
    "agent_output_dir = f'{base_path}/output'\n",
    "env_input_dir = agent_output_dir\n",
    "env_output_dir = agent_input_dir\n",
    "\n",
    "ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "nar_checkpoint = \"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ar_tokenizer = AutoTokenizer.from_pretrained(args.ar_checkpoint)\n",
    "ar_model = BartForConditionalGeneration.from_pretrained(args.ar_checkpoint)\n",
    "ar_model.to(device)\n",
    "\n",
    "dataset = load_from_disk(agent_input_dir)\n",
    "instruction_ids = ar_tokenizer(dataset[\"instruction\"][0])[\"input_ids\"][1 : -1]\n",
    "transcription_ids = ar_tokenizer(dataset[\"transcription\"][0])[\"input_ids\"][1 : -1]\n",
    "instruction = dataset[\"instruction\"][0]\n",
    "transcription = dataset[\"transcription\"][0]\n",
    "\n",
    "for i in range(len(instruction_ids)):\n",
    "    print(\"Instruction(cascade): \", ar_tokenizer.decode(instruction_ids[i]))\n",
    "for i in range(len(transcription_ids)):\n",
    "    print(\"Transcription(cascade): \", ar_tokenizer.decode(transcription_ids[i]))\n",
    "    \n",
    "observation_list = [{'input': 0, 'transcription': transcription, 'instruction': instruction, 'dataset': dataset}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VcActor' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-e62e93674809>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVcActor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mar_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnar_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_output_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-95a4579218f0>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, count)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mar_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;31m# use the model to predict the next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mlayer_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcascade_ar_nar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mar_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnar_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mar_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnar_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VcActor' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "\n",
    "actor = VcActor(model, tokenizer, ar_checkpoint, nar_checkpoint, dataset, agent_output_dir, device, observation_list, 100, 2)\n",
    "\n",
    "actor.predict(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0\n",
      "Dataset Dataset({\n",
      "    features: ['file_id', 'instruction', 'transcription', 'src_encodec_0', 'src_encodec_1', 'src_encodec_2', 'src_encodec_3', 'src_encodec_4', 'src_encodec_5', 'src_encodec_6', 'src_encodec_7', 'tgt_encodec_0', 'tgt_encodec_1', 'tgt_encodec_2', 'tgt_encodec_3', 'tgt_encodec_4', 'tgt_encodec_5', 'tgt_encodec_6', 'tgt_encodec_7'],\n",
      "    num_rows: 4980\n",
      "})\n",
      "[47302, 5, 723, 32407, 11, 10, 20228, 4737, 4]\n",
      "[113, 534, 5229, 11, 289, 13255, 523, 2901, 37, 15355, 4]\n",
      "Instruction:  Improve\n",
      "Instruction:   the\n",
      "Instruction:   higher\n",
      "Instruction:   frequencies\n",
      "Instruction:   in\n",
      "Instruction:   a\n",
      "Instruction:   noticeable\n",
      "Instruction:   manner\n",
      "Instruction:  .\n",
      "Transcription:  \"\n",
      "Transcription:  G\n",
      "Transcription:  ott\n",
      "Transcription:   in\n",
      "Transcription:   H\n",
      "Transcription:  imm\n",
      "Transcription:  el\n",
      "Transcription:  !\"\n",
      "Transcription:   he\n",
      "Transcription:   shouted\n",
      "Transcription:  .\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-fa926b92a0ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Step: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_ppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-96d75fa03f0c>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, count)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# use the model to predict the next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mlayer_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcascade_ar_nar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mar_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnar_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mar_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnar_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mencodec_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_encode_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnar_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynthesize_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencodec_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/b0990106x/TextRL/vc/trainer_encodec_vc_inference.py\u001b[0m in \u001b[0;36mcascade_ar_nar\u001b[0;34m(ar_model, nar_model, ar_tokenizer, nar_tokenizer, dataset, device)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnar_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstruction_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranscription_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_src_encodec_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1023\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mlayer_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnar_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnar_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnar_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/b0990106x/TextRL/vc/trainer_encodec_vc_inference.py\u001b[0m in \u001b[0;36mnar_decode\u001b[0;34m(model, tokenizer, inputs, batch_code, layer)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mbase_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mbase_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"decoder_input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mdecode_nar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbase_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     id_range_start, id_range_end = tokenizer.convert_tokens_to_ids(\n",
      "\u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1376\u001b[0m                 )\n\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1379\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1243\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/miniconda3/envs/textrl/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2235\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2236\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2237\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "# define env, actor, and agent\n",
    "env = VcRLEnv(model, tokenizer ,observation_input=observation_list, max_length=100, compare_sample=2, transcription=transcription, instruction=instruction)\n",
    "# actor = TextRLActor(env,model,tokenizer)\n",
    "\n",
    "actor = VcActor(model, tokenizer, ar_checkpoint, nar_checkpoint, dataset, agent_output_dir, device, observation_list, 100, 2)\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"Step: \", i)\n",
    "    actor.predict(i)\n",
    "    env.step(i)\n",
    "    actor.agent_ppo(update_interval=100, minibatch_size=3, epochs=10)\n",
    "    \n",
    "# agent = vc_inference.run(args, agent_input_dir, agent_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pfrl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-42dd87f82718>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m pfrl.experiments.train_agent_with_evaluation(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0meval_n_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pfrl' is not defined"
     ]
    }
   ],
   "source": [
    "agent = actor.agent_ppo(update_interval=100, minibatch_size=3, epochs=10)\n",
    "pfrl.experiments.train_agent_with_evaluation(\n",
    "    agent,\n",
    "    env,\n",
    "    steps=300,\n",
    "    eval_n_steps=None,\n",
    "    eval_n_episodes=1,       \n",
    "    train_max_episode_len=100,  \n",
    "    eval_interval=10,\n",
    "    outdir='elon_musk_dogecoin', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor.predict(observaton_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2. Environment to Agent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Model architecture: NISQA_DIM\n",
      "Loaded pretrained model from /work/b0990106x/TextRL/NISQA/weights/nisqa.tar\n",
      "reward 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/textrl/lib/python3.9/site-packages/librosa/filters.py:238: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# reward = env.get_reward()\n",
    "# print(\"reward\", reward)\n",
    "env._predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of agent (wav) + instruction + transcription"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
