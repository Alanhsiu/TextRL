Min index: 0, Max index: 51287
ar_tokenizer.vocab_size: 50265
inputs: {'input_ids': tensor([[    0, 20780,     5,  6086,  2330,     4,     2, 50673, 51100, 51100,
         51063, 50850, 50815, 50800, 50800, 51002, 51002, 50642, 50821, 50866,
         51052, 50273, 50364, 50676, 50676, 50643, 51202, 50643, 51202, 51069,
         51103, 51155, 51199, 50312, 50703, 50703, 50996, 51003, 50398, 50974,
         50744, 50744, 50744, 50416, 51205, 50767, 51171, 50672, 50910, 50335,
         50473, 50802, 50802, 51287, 50946, 50988, 51012, 50858, 51069, 50946,
         51144, 50401, 51232, 50498, 50696, 51019, 50686, 50447, 50447, 50916,
         51144, 51152, 51084, 51169, 51169, 51152, 50574, 51145, 50661, 51019,
         51040, 51262, 50487, 50601, 50813, 51106, 50534, 50744, 50744, 51205,
         50288, 50321, 51003, 51100, 50660, 50471, 51044, 50796, 51127, 51196,
         50571, 50468, 51020, 50634, 50271, 50731, 50981, 51213, 50347, 50840,
         50553, 50821, 51168, 50821, 50657, 51061, 51016, 51100, 50368, 50290,
         50673, 51100, 51100, 50604, 50604, 50660, 50515, 50971, 50582, 50744,
         51065, 51225, 50406, 50744, 51173, 51066, 50592, 51202, 50824, 50973,
         50637, 50637, 50838, 50702, 50702, 50686, 50468, 51004, 51095, 51004,
         50623, 51095, 50513, 50676, 50676, 50377, 50586, 50288, 50288, 50450,
         51236, 50327, 50604, 50726, 50753, 51199, 50413, 50638, 50826, 50946,
         51025, 50796, 50877, 50964, 50288, 51232, 50722, 51055, 50419, 51171,
         50730, 50767, 51149, 50744, 50511, 51085, 50866, 50574, 50981, 50579,
         50642, 50574, 50574, 50821, 50383, 50364, 50623, 51283, 51127, 51044,
         50327, 51100, 50290, 50519, 50519, 50942, 50338, 50408, 50961, 50961,
         50586, 51144, 50288, 50489, 50788, 50288, 51100, 51100, 51100, 50604,
         50324, 50522, 51084, 50737, 51100, 51100, 51100, 50604, 51100, 51100,
         50604, 50604, 50693, 51247, 51134, 50535, 50700, 50548, 51069, 51241,
         51140, 50863, 50618, 51125, 50674, 50676, 50866, 50915, 50760, 50327,
         51100, 51100, 50406, 51213, 50347, 50679, 50923, 50586, 50489, 50586,
         51196, 50268, 50364, 50273, 50485, 51040, 51004, 51135, 51095, 51004,
         51095, 50960, 50960, 50969, 50473, 51125, 51266, 51247, 50505, 50858,
         51095, 50676, 50328, 51120, 51282, 51100, 51100, 50695, 50604, 50604,
         50604, 50519, 50519, 50303, 50942, 50338, 51133, 50863, 50828, 51155,
         50863, 50489, 50863, 50941, 51125, 50900, 50575, 50473, 50489, 50941,
         50756, 50586, 50964, 50401, 50697, 51284, 50740, 50802, 50441, 50441,
         50701, 50638, 50425, 50974, 50604, 50604, 50604, 50604, 50740, 50802,
         51282, 51100, 51100, 51100,     2]], device='cuda:0')}
ar_tokenizer: BartTokenizerFast(
    name_or_path='lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans', 
    vocab_size=50265, 
    model_max_length=1024, 
    is_fast=True, 
    padding_side='right', 
    truncation_side='right', 
    special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken("<mask>", rstrip=False, lstrip=True, single_word=False, normalized=False)}, 
    clean_up_tokenization_spaces=True)

for token, index in ar_tokenizer.vocab.items():
    print(f'Token: {token}, Index: {index}')